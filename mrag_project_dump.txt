# ===============================================
# M-RAG FULL PROJECT DUMP
# Generated automatically - ready to share
# ===============================================



=== FILE: mrag_project_dump.txt ===
============================================================

============================================================


=== FILE: Readme.md ===
============================================================

============================================================


=== FILE: pyproject.toml ===
============================================================
[project]
name = "m-rag"
version = "0.1.0"
description = "Multimodal RAG Framework — PDF + Image ingestion with LangGraph"
requires-python = ">=3.12"
dependencies = [
    # LLM / LangChain
    "langchain",
    "langchain-groq",
    "langchain-community",
    "langgraph",
    # Vector store
    "qdrant-client",
    "langchain-qdrant",
    # Postgres / ORM
    "sqlmodel",
    "asyncpg",
    "psycopg2-binary",
    # Embeddings
    "sentence-transformers",
    # Document parsing
    # "unstructured[pdf,image]",
    "pymupdf",
    "pillow",
    # Observability
    "langfuse",
    # CLI
    "typer[standard]",
    "rich",
    # Async / utilities
    "httpx",
    "python-dotenv",
    "pydantic",
    "pydantic-settings",
    "tenacity",
    # Reranker
    "sentence-transformers @ git+https://github.com/UKPLab/sentence-transformers",]

[project.scripts]
m-rag = "cli.main:app"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["cli", "core", "ingestion", "rag", "scripts", "utils"]

[tool.hatch.metadata]
allow-direct-references = true

[tool.uv]
dev-dependencies = [
    "pytest",
    "pytest-asyncio",
    "httpx",
    "ruff",
    "mypy",
]

[tool.ruff]
line-length = 100
target-version = "py311"

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[tool.uv.sources]
torch = { index = "pytorch-cpu" }
torchvision = { index = "pytorch-cpu" }
============================================================


=== FILE: docker-compose.yml ===
============================================================
services:
  db-postgres:
    image: postgres:16-alpine
    container_name: rag-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: mrag
      POSTGRES_USER: mrag
      POSTGRES_PASSWORD: mrag_secret
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage
volumes:

  postgres_data:
  qdrant_storage:
============================================================


=== FILE: api/rag_router.py ===
============================================================
"""
api/rag_router.py
=================
FastAPI router for agentic RAG queries.

Endpoints
---------
POST /rag/query          Submit a question (async background job)
GET  /rag/query/{job_id} Poll job status + partial results
POST /rag/query/sync     Synchronous query (waits for result, ≤60s)
GET  /rag/strategies     List available strategy names
"""
from __future__ import annotations

import asyncio
import logging
import uuid
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel, Field

from rag.config import RAGConfig

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/rag", tags=["RAG"])

# In-memory job store (swap for Redis in production)
_rag_jobs: Dict[str, Dict[str, Any]] = {}


# ─────────────────────────────────────────────────────────────────────────────
# Request / response models
# ─────────────────────────────────────────────────────────────────────────────
class QueryRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=2000)
    config: Optional[RAGConfig] = None


class QueryResponse(BaseModel):
    job_id: str
    status: str
    message: str


class QueryResult(BaseModel):
    job_id: str
    status: str                           # running | success | failed | max_iterations | no_store
    query: str
    final_answer: Optional[str] = None
    answer_sources: List[str] = []
    answer_confidence: Optional[float] = None
    iterations_used: Optional[int] = None
    notepad: Optional[List[Dict]] = None  # iteration log
    probe_diagnostics: Optional[str] = None   # store probe summary
    store_ready: Optional[bool] = None
    error: Optional[str] = None


# ─────────────────────────────────────────────────────────────────────────────
# Background worker
# ─────────────────────────────────────────────────────────────────────────────
async def _run_rag_job(job_id: str, query: str, config: Dict[str, Any]) -> None:
    _rag_jobs[job_id]["status"] = "running"
    try:
        from rag.graph import run_rag
        result = await run_rag(query=query, config=config)
        _rag_jobs[job_id].update(
            {
                "status": result.get("status", "success"),
                "final_answer": result.get("final_answer"),
                "answer_sources": result.get("answer_sources", []),
                "answer_confidence": result.get("answer_confidence"),
                "iterations_used": result.get("iteration"),
                "notepad": result.get("notepad"),
                "probe_diagnostics": result.get("probe_diagnostics"),
                "store_ready": result.get("store_ready"),
                "error": result.get("error"),
            }
        )
    except Exception as exc:
        logger.error(f"RAG job {job_id} failed: {exc}", exc_info=True)
        _rag_jobs[job_id].update({"status": "failed", "error": str(exc)})


# ─────────────────────────────────────────────────────────────────────────────
# Routes
# ─────────────────────────────────────────────────────────────────────────────
@router.get("/strategies")
async def list_strategies():
    from rag.strategies import ALL_STRATEGIES
    return {"strategies": ALL_STRATEGIES}


@router.post("/query", response_model=QueryResponse, status_code=202)
async def submit_query(request: QueryRequest, background_tasks: BackgroundTasks):
    """Submit a RAG query as a background job."""
    config = (request.config or RAGConfig()).model_dump()
    job_id = str(uuid.uuid4())
    _rag_jobs[job_id] = {
        "status": "queued",
        "query": request.query,
        "final_answer": None,
        "answer_sources": [],
        "answer_confidence": None,
        "iterations_used": None,
        "notepad": None,
        "error": None,
    }
    background_tasks.add_task(_run_rag_job, job_id, request.query, config)
    return QueryResponse(
        job_id=job_id,
        status="queued",
        message=f"RAG job queued for query: {request.query[:60]}…",
    )


@router.get("/query/{job_id}", response_model=QueryResult)
async def get_query_result(job_id: str):
    """Poll a RAG job."""
    job = _rag_jobs.get(job_id)
    if job is None:
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' not found")
    return QueryResult(job_id=job_id, query=job.get("query", ""), **{
        k: job.get(k) for k in (
            "status", "final_answer", "answer_sources",
            "answer_confidence", "iterations_used", "notepad",
            "probe_diagnostics", "store_ready", "error"
        )
    })


@router.post("/query/sync", response_model=QueryResult)
async def sync_query(request: QueryRequest):
    """
    Synchronous RAG query — waits up to 120s for the result.
    Suitable for direct API calls and testing.
    """
    config = (request.config or RAGConfig()).model_dump()
    job_id = str(uuid.uuid4())
    _rag_jobs[job_id] = {"status": "running", "query": request.query}

    try:
        from rag.graph import run_rag
        result = await asyncio.wait_for(
            run_rag(query=request.query, config=config),
            timeout=120,
        )
        return QueryResult(
            job_id=job_id,
            query=request.query,
            status=result.get("status", "success"),
            final_answer=result.get("final_answer"),
            answer_sources=result.get("answer_sources", []),
            answer_confidence=result.get("answer_confidence"),
            iterations_used=result.get("iteration"),
            notepad=result.get("notepad"),
            error=result.get("error"),
        )
    except asyncio.TimeoutError:
        raise HTTPException(status_code=504, detail="RAG query timed out (120s)")
    except Exception as exc:
        raise HTTPException(status_code=500, detail=str(exc))
============================================================


=== FILE: api/main.py ===
============================================================
"""
api/main.py
===========
FastAPI backend for M-RAG with permanent image storage.
Images are saved in ./uploads/ and served at http://localhost:8000/uploads/...
"""

from __future__ import annotations

import json
import logging
import os
import shutil
import uuid
from typing import Any, Dict, Optional

from dotenv import load_dotenv
load_dotenv()

from fastapi import BackgroundTasks, FastAPI, File, Form, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# Project imports
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from core.config import PipelineConfig
from core.settings import get_settings

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(name)s | %(message)s")

app = FastAPI(
    title="M-RAG Ingestion API",
    version="1.0.0",
    description="Multimodal RAG document ingestion pipeline",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ====================== PERMANENT STORAGE ======================
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

# Mount RAG router
from api.rag_router import router as rag_router
app.include_router(rag_router)

# ====================== IN-MEMORY JOB STORE ======================
_jobs: Dict[str, Dict[str, Any]] = {}

# ====================== RESPONSE MODELS ======================
class IngestResponse(BaseModel):
    job_id: str
    status: str
    message: str


class JobStatus(BaseModel):
    job_id: str
    status: str          # queued | running | success | failed
    document_id: Optional[str] = None
    metrics: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


class CollectionsResponse(BaseModel):
    collections: list[str]


# ====================== BACKGROUND WORKER ======================
async def _run_pipeline(job_id: str, file_path: str, config: Dict[str, Any]) -> None:
    _jobs[job_id]["status"] = "running"
    try:
        from ingestion.graph import run_ingestion
        final_state = await run_ingestion(
            file_path=file_path,
            config=config,
            run_name=f"job_{job_id}",
        )
        _jobs[job_id].update({
            "status": final_state.get("status", "success"),
            "document_id": final_state.get("document_id"),
            "metrics": final_state.get("metrics"),
            "error": final_state.get("error"),
        })
    except Exception as exc:
        logger.error(f"Job {job_id} failed: {exc}", exc_info=True)
        _jobs[job_id].update({"status": "failed", "error": str(exc)})
    # NO os.remove() — files stay forever in ./uploads/


# ====================== ROUTES ======================
@app.get("/health")
async def health():
    return {"status": "ok"}


@app.get("/config/defaults", response_model=PipelineConfig)
async def get_default_config():
    """Return the default PipelineConfig (seeded from env vars)."""
    settings = get_settings()
    return PipelineConfig.from_settings(settings)


@app.get("/collections", response_model=CollectionsResponse)
async def list_collections():
    """List all Qdrant collections."""
    try:
        from qdrant_client import QdrantClient
        settings = get_settings()
        client = QdrantClient(
            host=settings.qdrant_host,
            port=settings.qdrant_port,
            api_key=settings.qdrant_api_key,
            https=False,   # matches your local setup
        )
        names = [c.name for c in client.get_collections().collections]
        return CollectionsResponse(collections=names)
    except Exception as exc:
        raise HTTPException(status_code=503, detail=f"Qdrant unreachable: {exc}")


@app.post("/ingest", response_model=IngestResponse, status_code=202)
async def ingest(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    config_json: Optional[str] = Form(None),
):
    """Upload a document and kick off ingestion."""
    # Validate file type
    allowed_exts = {".pdf", ".txt", ".md", ".png", ".jpg", ".jpeg"}
    _, ext = os.path.splitext(file.filename.lower())
    if ext not in allowed_exts:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type '{ext}'. Allowed: {allowed_exts}",
        )

    # Parse config
    settings = get_settings()
    base_config = PipelineConfig.from_settings(settings)

    if config_json:
        try:
            overrides = json.loads(config_json)
            config = base_config.model_copy(update=overrides)
        except Exception as exc:
            raise HTTPException(status_code=422, detail=f"Invalid config JSON: {exc}")
    else:
        config = base_config

    # Inject infra secrets
    config_dict = config.model_dump()
    config_dict["qdrant_host"] = settings.qdrant_host
    config_dict["qdrant_port"] = settings.qdrant_port
    config_dict["qdrant_api_key"] = settings.qdrant_api_key

    # === SAVE TO PERMANENT FOLDER (never deleted) ===
    file_id = str(uuid.uuid4())
    safe_filename = "".join(c for c in file.filename if c.isalnum() or c in "._-")
    permanent_path = os.path.join(UPLOAD_DIR, f"{file_id}_{safe_filename}")

    with open(permanent_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    # Create job
    job_id = str(uuid.uuid4())
    _jobs[job_id] = {
        "status": "queued",
        "document_id": None,
        "metrics": None,
        "error": None,
    }

    background_tasks.add_task(_run_pipeline, job_id, permanent_path, config_dict)

    return IngestResponse(
        job_id=job_id,
        status="queued",
        message=f"Ingestion queued for '{file.filename}'",
    )


@app.get("/ingest/{job_id}", response_model=JobStatus)
async def get_job_status(job_id: str):
    """Poll the status of an ingestion job."""
    job = _jobs.get(job_id)
    if job is None:
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' not found")
    return JobStatus(job_id=job_id, **job)


# ====================== DEV RUNNER ======================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("api.main:app", host="0.0.0.0", port=8000, reload=True)
============================================================


=== FILE: experiments/dump_project.py ===
============================================================
# scripts/dump_project.py
# Run this from your project root: python scripts/dump_project.py
# It will create mrag_project_dump.txt (one single file with everything)

import os
from pathlib import Path

def should_include_file(file_path: str) -> bool:
    """Only include text/source files, skip binaries and junk."""
    ext = Path(file_path).suffix.lower()
    text_exts = {
        '.py', '.md', '.toml', '.yaml', '.yml', '.txt',
        '.json', '.env', '.example', '.gitignore', '.pre-commit-config.yaml'
    }
    return ext in text_exts

def should_skip_dir(dir_name: str) -> bool:
    skip = {
        '__pycache__',
        'models-cache',
        '.git',
        'venv',
        'env',
        '.venv',
        'node_modules',
        '__pycache__',
        '.pytest_cache',
        '.ruff_cache',
        'dist',
        'build'
    }
    return dir_name in skip or dir_name.startswith('.')

def dump_project(output_file: str = "mrag_project_dump.txt"):
    root = os.getcwd()
    total_files = 0
    
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("# ===============================================\n")
        f.write("# M-RAG FULL PROJECT DUMP\n")
        f.write("# Generated automatically - ready to share\n")
        f.write("# ===============================================\n\n")
        
        for dirpath, dirnames, filenames in os.walk(root):
            # Remove directories we want to skip
            dirnames[:] = [d for d in dirnames if not should_skip_dir(d)]
            
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                rel_path = os.path.relpath(filepath, root)
                
                if should_include_file(filepath):
                    try:
                        with open(filepath, "r", encoding="utf-8") as src:
                            content = src.read()
                        
                        f.write(f"\n\n=== FILE: {rel_path} ===\n")
                        f.write("=" * 60 + "\n")
                        f.write(content)
                        f.write("\n" + "=" * 60 + "\n")
                        total_files += 1
                        print(f"✓ {rel_path}")
                    except Exception as e:
                        print(f"⚠️  Skipped (encoding issue): {rel_path}")
    
    print(f"\n✅ DONE! Created {output_file} with {total_files} files.")
    print("   Just share this one file with me or Claude.")

if __name__ == "__main__":
    dump_project()
============================================================


=== FILE: experiments/sb.py ===
============================================================
"""Sandbox / evaluation script — run quick RAG experiments."""

from __future__ import annotations

import asyncio
import json
import os
import sys

sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from rag.graph import run_rag

# ── Simple eval set ───────────────────────────────────────────────────────────
QUESTIONS = [
    "What is the main topic of the document?",
    "Summarize the key findings.",
    "What numbers or statistics are mentioned?",
]


async def evaluate(collection: str = "m_rag") -> dict:
    results = []
    for q in QUESTIONS:
        state = await run_rag(q, collection=collection)
        results.append({
            "question": q,
            "answer": state.get("answer", ""),
            "sources": state.get("sources", []),
        })
        print(f"\nQ: {q}\nA: {state.get('answer', '')}\n")

    metrics = {
        "total_questions": len(results),
        "answered": sum(1 for r in results if r["answer"]),
    }

    os.makedirs("metrics", exist_ok=True)
    with open("metrics/eval.json", "w") as f:
        json.dump(metrics, f, indent=2)

    print("\nMetrics:", metrics)
    return metrics


if __name__ == "__main__":
    collection = sys.argv[1] if len(sys.argv) > 1 else "m_rag"
    asyncio.run(evaluate(collection))
============================================================


=== FILE: cli/main.py ===
============================================================
"""M-RAG CLI — ingest · chat · status"""

from __future__ import annotations
import os

os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import asyncio
from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.table import Table

from ingestion.graph import run_ingestion
# from rag.graph import run_rag
from utils.logger import get_logger

from sqlalchemy import text
import httpx

import logging
from typing import Optional
from pathlib import Path
import asyncio
import typer
from rich.console import Console

from dotenv import load_dotenv
load_dotenv()


app = typer.Typer(name="m-rag", add_completion=False, rich_markup_mode="rich")
console = Console()
# Add this to your setup_logging function


def setup_logging(level: str):
    """Configures the global logging level."""
    
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        numeric_level = logging.INFO
        
    logging.basicConfig(
        level=numeric_level,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%H:%M:%S",
    )
    # Optional: Silence noisy 3rd party libraries
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("asyncio").setLevel(logging.WARNING)
    # 1. Silence the 'transformers' logger
    logging.getLogger("transformers").setLevel(logging.ERROR)
    logging.getLogger("sentence_transformers").setLevel(logging.ERROR)
    logging.getLogger("core").setLevel(numeric_level)
    logging.getLogger("ingestion").setLevel(numeric_level)

    # 2. Disable the progress bars (the 'Loading weights' lines)
    # This environment variable tells Hugging Face to be quiet
    os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
    
    # 3. Optional: Silence noisy network logs from Qdrant/HTTPX
    logging.getLogger("httpx").setLevel(logging.WARNING)

@app.callback()
def main(
    verbose: str = typer.Option(
        "INFO", 
        "--log", "-l", 
        help="Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL"
    ),
):
    """M-RAG CLI: Multimodal RAG Ingestion & Chat."""
    setup_logging(verbose)






# ── ingest ────────────────────────────────────────────────────────────────────

@app.command()
def ingest(
    path: Path = typer.Argument(..., help="PDF or image file / directory to ingest"),
    collection: str = typer.Option("m_rag", "--collection", "-c", help="Qdrant collection"),
) -> None:
    """Ingest a PDF or image into the vector store."""
    
    # We use a context manager to show a spinner while the log stream runs in the background
    with console.status("[bold str]Processing pipeline...") as status:
        try:
            config={
        "describe_images": True,
        "chunking_method": "recursive",   # or "semantic", "recursive"
        "embedding_type": "multimodal",      # best for mixed content
        "collection_name": collection,
    }
            result = asyncio.run(run_ingestion(str(path), config=config))
            console.print(f"\n[green]✓[/green] Ingestion Complete. Final Status: [bold]{result.get('status')}[/bold]")
        except Exception as e:
            console.print(f"\n[red]✗[/red] Ingestion Failed: {e}")
            raise typer.Exit(code=1)
    
# ── chat ──────────────────────────────────────────────────────────────────────

# @app.command()
# def chat(
#     query: str = typer.Argument(..., help="Your question"),
#     session_id: Optional[str] = typer.Option(None, "--session-id", "-s"),
#     collection: str = typer.Option("m_rag", "--collection", "-c"),
#     top_k: int = typer.Option(5, "--top-k", "-k", help="Chunks to retrieve"),
# ) -> None:
#     """Ask a question against the ingested knowledge base."""
#     tracer = get_tracer()
#     with tracer.trace("cli.chat", input={"query": query, "session_id": session_id}):
#         console.print(f"[cyan]Thinking…[/cyan]")
#         result = asyncio.run(
#             run_rag(query, session_id=session_id, collection=collection, top_k=top_k)
#         )
#         console.print("\n[bold green]Answer:[/bold green]")
#         console.print(result["answer"])
#         if result.get("sources"):
#             console.print("\n[dim]Sources:[/dim]")
#             for src in result["sources"]:
#                 console.print(f"  • {src}")


# ── status ────────────────────────────────────────────────────────────────────

@app.command()
def status(
    as_json: bool = typer.Option(False, "--json", help="Output as JSON"),
) -> None:
    """Show health of all system components."""
    import json
    from qdrant_client import QdrantClient
    from core.databases import check_db_connection
    import os
    from langfuse import Langfuse

    health: dict[str, str] = {}

    # Qdrant
    try:
        qc = QdrantClient(url=os.getenv("QDRANT_URL", "http://localhost:6333"))
        qc.get_collections()
        health["qdrant"] = "ok"
    except Exception as e:
        health["qdrant"] = f"error: {e}"

    # Postgres
    try:
        asyncio.run(check_db_connection())
        health["postgres"] = "ok"
    except Exception as e:
        health["postgres"] = f"error: {e}"

    try:
        # This checks if the SDK can actually authenticate with your keys
        lf = Langfuse(
            public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
            secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
            host=os.getenv("LANGFUSE_HOST", "http://localhost:3000")
        )
        if lf.auth_check():
            health["langfuse"] = "ok"
        else:
            health["langfuse"] = "error: auth failed"
    except Exception:
        health["langfuse"] = "error: unreachable"

    # --- Render ---
    if as_json:
        console.print_json(json.dumps(health))
    else:
        table = Table(title="M-RAG System Status")
        table.add_column("Component", style="cyan")
        table.add_column("Status", style="bold")
        for component, stat in health.items():
            colour = "green" if stat == "ok" else "red"
            table.add_row(component, f"[{colour}]{stat}[/{colour}]")
        console.print(table)


if __name__ == "__main__":
    app()
============================================================


=== FILE: utils/utils.py ===
============================================================
"""Misc helpers — file handling, ID generation, etc."""

from __future__ import annotations

import hashlib
import uuid
from pathlib import Path


def generate_id() -> str:
    """Generate a random UUID string."""
    return str(uuid.uuid4())


def file_hash(path: str | Path, algo: str = "sha256") -> str:
    """Return hex digest of a file for deduplication."""
    h = hashlib.new(algo)
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(65536), b""):
            h.update(chunk)
    return h.hexdigest()


def ensure_dir(path: str | Path) -> Path:
    """Create directory (and parents) if missing; return Path."""
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p


def truncate_text(text: str, max_chars: int = 500) -> str:
    """Truncate text to max_chars for display purposes."""
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "…"
============================================================


=== FILE: utils/logger.py ===
============================================================
"""Centralized logger setup."""

from __future__ import annotations

import logging
import os

LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO").upper()

logging.basicConfig(
    level=LOG_LEVEL,
    format="%(asctime)s | %(levelname)-8s | %(name)s | %(message)s",
    datefmt="%Y-%m-%dT%H:%M:%S",
)


def get_logger(name: str) -> logging.Logger:
    """Return a named logger at the configured level."""
    logger = logging.getLogger(name)
    logger.setLevel(LOG_LEVEL)
    return logger
============================================================


=== FILE: tracing_langfuse/docker-compose.yml ===
============================================================
# Make sure to update the credential placeholders with your own secrets.
# We mark them with # CHANGEME in the file below.
# In addition, we recommend to restrict inbound traffic on the host to langfuse-web (port 3000) and minio (port 9090) only.
# All other components are bound to localhost (127.0.0.1) to only accept connections from the local machine.
# External connections from other machines will not be able to reach these services directly.
services:
  langfuse-worker:
    image: docker.io/langfuse/langfuse-worker:3
    restart: always
    depends_on: &langfuse-depends-on
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    ports:
      - 127.0.0.1:3030:3030
    environment: &langfuse-worker-env
      NEXTAUTH_URL: ${NEXTAUTH_URL:-http://localhost:3000}
      DATABASE_URL: ${DATABASE_URL:-postgresql://postgres:postgres@postgres:5432/postgres} # CHANGEME
      SALT: ${SALT:-mysalt} # CHANGEME
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:-0000000000000000000000000000000000000000000000000000000000000000} # CHANGEME: generate via `openssl rand -hex 32`
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-true}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: ${LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES:-false}
      CLICKHOUSE_MIGRATION_URL: ${CLICKHOUSE_MIGRATION_URL:-clickhouse://clickhouse:9000}
      CLICKHOUSE_URL: ${CLICKHOUSE_URL:-http://clickhouse:8123}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse} # CHANGEME
      CLICKHOUSE_CLUSTER_ENABLED: ${CLICKHOUSE_CLUSTER_ENABLED:-false}
      LANGFUSE_USE_AZURE_BLOB: ${LANGFUSE_USE_AZURE_BLOB:-false}
      LANGFUSE_S3_EVENT_UPLOAD_BUCKET: ${LANGFUSE_S3_EVENT_UPLOAD_BUCKET:-langfuse}
      LANGFUSE_S3_EVENT_UPLOAD_REGION: ${LANGFUSE_S3_EVENT_UPLOAD_REGION:-auto}
      LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID: ${LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY: ${LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT: ${LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT:-http://minio:9000}
      LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE: ${LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE:-true}
      LANGFUSE_S3_EVENT_UPLOAD_PREFIX: ${LANGFUSE_S3_EVENT_UPLOAD_PREFIX:-events/}
      LANGFUSE_S3_MEDIA_UPLOAD_BUCKET: ${LANGFUSE_S3_MEDIA_UPLOAD_BUCKET:-langfuse}
      LANGFUSE_S3_MEDIA_UPLOAD_REGION: ${LANGFUSE_S3_MEDIA_UPLOAD_REGION:-auto}
      LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID: ${LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY: ${LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT: ${LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT:-http://localhost:9090}
      LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE: ${LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE:-true}
      LANGFUSE_S3_MEDIA_UPLOAD_PREFIX: ${LANGFUSE_S3_MEDIA_UPLOAD_PREFIX:-media/}
      LANGFUSE_S3_BATCH_EXPORT_ENABLED: ${LANGFUSE_S3_BATCH_EXPORT_ENABLED:-false}
      LANGFUSE_S3_BATCH_EXPORT_BUCKET: ${LANGFUSE_S3_BATCH_EXPORT_BUCKET:-langfuse}
      LANGFUSE_S3_BATCH_EXPORT_PREFIX: ${LANGFUSE_S3_BATCH_EXPORT_PREFIX:-exports/}
      LANGFUSE_S3_BATCH_EXPORT_REGION: ${LANGFUSE_S3_BATCH_EXPORT_REGION:-auto}
      LANGFUSE_S3_BATCH_EXPORT_ENDPOINT: ${LANGFUSE_S3_BATCH_EXPORT_ENDPOINT:-http://minio:9000}
      LANGFUSE_S3_BATCH_EXPORT_EXTERNAL_ENDPOINT: ${LANGFUSE_S3_BATCH_EXPORT_EXTERNAL_ENDPOINT:-http://localhost:9090}
      LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID: ${LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_BATCH_EXPORT_SECRET_ACCESS_KEY: ${LANGFUSE_S3_BATCH_EXPORT_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE: ${LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE:-true}
      LANGFUSE_INGESTION_QUEUE_DELAY_MS: ${LANGFUSE_INGESTION_QUEUE_DELAY_MS:-}
      LANGFUSE_INGESTION_CLICKHOUSE_WRITE_INTERVAL_MS: ${LANGFUSE_INGESTION_CLICKHOUSE_WRITE_INTERVAL_MS:-}
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      REDIS_AUTH: ${REDIS_AUTH:-myredissecret} # CHANGEME
      REDIS_TLS_ENABLED: ${REDIS_TLS_ENABLED:-false}
      REDIS_TLS_CA: ${REDIS_TLS_CA:-/certs/ca.crt}
      REDIS_TLS_CERT: ${REDIS_TLS_CERT:-/certs/redis.crt}
      REDIS_TLS_KEY: ${REDIS_TLS_KEY:-/certs/redis.key}
      EMAIL_FROM_ADDRESS: ${EMAIL_FROM_ADDRESS:-}
      SMTP_CONNECTION_URL: ${SMTP_CONNECTION_URL:-}

  langfuse-web:
    image: docker.io/langfuse/langfuse:3
    restart: always
    depends_on: *langfuse-depends-on
    ports:
      - 3000:3000
    environment:
      <<: *langfuse-worker-env
      NEXTAUTH_SECRET: ${NEXTAUTH_SECRET:-mysecret} # CHANGEME
      LANGFUSE_INIT_ORG_ID: ${LANGFUSE_INIT_ORG_ID:-}
      LANGFUSE_INIT_ORG_NAME: ${LANGFUSE_INIT_ORG_NAME:-}
      LANGFUSE_INIT_PROJECT_ID: ${LANGFUSE_INIT_PROJECT_ID:-}
      LANGFUSE_INIT_PROJECT_NAME: ${LANGFUSE_INIT_PROJECT_NAME:-}
      LANGFUSE_INIT_PROJECT_PUBLIC_KEY: ${LANGFUSE_INIT_PROJECT_PUBLIC_KEY:-}
      LANGFUSE_INIT_PROJECT_SECRET_KEY: ${LANGFUSE_INIT_PROJECT_SECRET_KEY:-}
      LANGFUSE_INIT_USER_EMAIL: ${LANGFUSE_INIT_USER_EMAIL:-}
      LANGFUSE_INIT_USER_NAME: ${LANGFUSE_INIT_USER_NAME:-}
      LANGFUSE_INIT_USER_PASSWORD: ${LANGFUSE_INIT_USER_PASSWORD:-}

  clickhouse:
    image: docker.io/clickhouse/clickhouse-server
    restart: always
    user: "101:101"
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse} # CHANGEME
    volumes:
      - langfuse_clickhouse_data:/var/lib/clickhouse
      - langfuse_clickhouse_logs:/var/log/clickhouse-server
    ports:
      - 127.0.0.1:8123:8123
      - 127.0.0.1:9000:9000
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 1s

  minio:
    image: cgr.dev/chainguard/minio
    restart: always
    entrypoint: sh
    # create the 'langfuse' bucket before starting the service
    command: -c 'mkdir -p /data/langfuse && minio server --address ":9000" --console-address ":9001" /data'
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-miniosecret} # CHANGEME
    ports:
      - 9090:9000
      - 127.0.0.1:9091:9001
    volumes:
      - langfuse_minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 1s
      timeout: 5s
      retries: 5
      start_period: 1s

  redis:
    image: docker.io/redis:7
    restart: always
    # CHANGEME: row below to secure redis password
    command: >
      --requirepass ${REDIS_AUTH:-myredissecret}
      --maxmemory-policy noeviction
    ports:
      - 127.0.0.1:6379:6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 3s
      timeout: 10s
      retries: 10

  postgres:
    image: docker.io/postgres:${POSTGRES_VERSION:-17}
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 3s
      timeout: 3s
      retries: 10
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres} # CHANGEME
      POSTGRES_DB: ${POSTGRES_DB:-postgres}
      TZ: UTC
      PGTZ: UTC
    ports:
      - 127.0.0.1:5432:5432
    volumes:
      - langfuse_postgres_data:/var/lib/postgresql/data

volumes:
  langfuse_postgres_data:
    driver: local
  langfuse_clickhouse_data:
    driver: local
  langfuse_clickhouse_logs:
    driver: local
  langfuse_minio_data:
    driver: local

============================================================


=== FILE: core/mrag_project_dump.txt ===
============================================================
# ===============================================
# M-RAG FULL PROJECT DUMP
# Generated automatically - ready to share
# ===============================================



=== FILE: mrag_project_dump.txt ===
============================================================

============================================================


=== FILE: __init__.py ===
============================================================
from .state import IngestionState
from .elements import (
    BaseElement, TextElement, ImageElement,
    TableElement
)

__all__ = [
    "IngestionState",
    "BaseElement", "TextElement", "ImageElement",
    "TableElement",
]

============================================================


=== FILE: state.py ===
============================================================
from typing import TypedDict, List, Annotated, Dict, Any, Union
import operator
from .elements import BaseElement

class IngestionState(TypedDict):
    """
    The state object passed between LangGraph nodes.
    """
    file_path: str
    elements: Annotated[List[BaseElement], operator.add]
    config: Dict[str, Any] 
    next_step: str
    metrics: Dict[str, Any]
    document_id: str



# import logging
# import uuid
# from typing import Annotated, List, Dict, Any, Optional
# from typing_extensions import TypedDict
# from pydantic import BaseModel, Field

# # Setup module-level logger
# logger = logging.getLogger(__name__)

# # --- Models ---
# class BaseElement(BaseModel):
#     element_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
#     type: str  
#     content: Any               
#     metadata: Dict[str, Any] = Field(default_factory=dict)
#     children: List["BaseElement"] = Field(default_factory=list)

# # --- Reducers (The logical heart of the state) ---

# def reduce_elements(left: List[BaseElement], right: List[BaseElement]) -> List[BaseElement]:
#     """Append + dedupe logic with visibility into merge conflicts."""
#     if not right:
#         return left
    
#     seen = {e.element_id for e in left}
#     new_elements = []
#     duplicates = 0

#     for e in right:
#         if e.element_id not in seen:
#             new_elements.append(e)
#             seen.add(e.element_id)
#         else:
#             duplicates += 1

#     if duplicates > 0:
#         logger.debug(f"[REDUCER] Deduplicated {duplicates} elements during state merge.")
    
#     if new_elements:
#         logger.debug(f"[REDUCER] Added {len(new_elements)} new elements to state.")

#     return left + new_elements

# def reduce_chunks(left: List[BaseElement], right: List[BaseElement]) -> List[BaseElement]:
#     """Same as elements, but often where parent-child mapping errors occur."""
#     return reduce_elements(left, right)

# def reduce_errors(left: List[str], right: List[str]) -> List[str]:
#     """Log errors as they are appended to the global state."""
#     for error in right:
#         logger.error(f"[STATE ERROR] {error}")
#     return (left or []) + (right or [])

# # --- IngestionState ---

# class IngestionState(TypedDict, total=False):
#     file_path: str
#     document_id: str
#     file_type: str
#     elements: List[BaseElement]
#     chunks: List[BaseElement]
#     embeddings: List[Dict]
#     metadata: Dict[str, Any]
#     status: str
#     errors: List[str]
#     embedding_config: Dict[str, str]

# # State with reducers for LangGraph
# StateSchema = Annotated[IngestionState, {
#     "elements": reduce_elements,
#     "chunks": reduce_chunks,
#     "errors": reduce_errors,
# }]

# class RAGState(TypedDict, total=False):
#     """State flowing through the RAG graph."""
#     query: str                       # user question
#     session_id: Optional[str]        # conversation session
#     collection: str                  # Qdrant collection to search
#     top_k: int                       # number of chunks to retrieve
#     query_embedding: list[float]     # embedded query vector
#     retrieved_chunks: list[dict]     # raw retrieval results
#     reranked_chunks: list[dict]      # reranked subset
#     context: str                     # formatted context string
#     answer: str                      # final LLM answer
#     sources: list[str]               # source file references
#     error: Optional[str]
============================================================


=== FILE: elements.py ===
============================================================
# import logging
# from pydantic import BaseModel, Field
# from typing import List, Dict, Any, Literal, Union, Optional
# from PIL import Image
# import uuid

# # Get logger for domain models
# logger = logging.getLogger(__name__)

# ElementType = Literal["text", "image", "table", "audio"]

# class BaseElement(BaseModel):
#     element_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
#     type: ElementType
#     content: Any
#     metadata: Dict[str, Any] = Field(default_factory=dict)
#     children: List["BaseElement"] = Field(default_factory=list)

#     embedding: Optional[List[float]] = None
#     embedding_model: Optional[str] = None

#     def get_text_for_retrieval(self) -> str:
#         """Unified text representation logic with logging for debugging retrieval paths."""
        
#         # 1. Check for processed multimodal descriptions
#         if isinstance(self, ImageElement) and self.description:
#             logger.debug(f"Retrieval: Using description for Image {self.element_id}")
#             return self.description
            
#         if isinstance(self, AudioElement) and self.transcription:
#             logger.debug(f"Retrieval: Using transcription for Audio {self.element_id}")
#             return self.transcription
            
#         # 2. Check for raw text content
#         if isinstance(self.content, str):
#             # We don't log here to avoid flooding, as this is the standard path
#             return self.content
            
#         # 3. Fallback path (potential quality issue)
#         logger.warning(
#             f"Retrieval fallback: Element {self.element_id} (type: {self.type}) "
#             f"has no specialized text. Using str() conversion."
#         )
#         return str(self.content)

#     class Config:
#         arbitrary_types_allowed = True 

# # --- Subclasses (Keep these as pure schemas) ---

# class TextElement(BaseElement):
#     type: ElementType = "text"
#     content: str

# class ImageElement(BaseElement):
#     type: ElementType = "image"
#     content: Image.Image
#     description: str = ""

# class TableElement(BaseElement):
#     type: ElementType = "table"
#     content: str # markdown

# class AudioElement(BaseElement):
#     type: ElementType = "audio"
#     content: str
#     transcription: str = ""


from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
from uuid import uuid4
from PIL import Image

@dataclass
class BaseElement:
    element_id: str = field(default_factory=lambda: str(uuid4()))
    content: Any = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    children: List["BaseElement"] = field(default_factory=list) # For Hierarchical chunking

    def to_dict(self) -> Dict[str, Any]:
        """Convert element to dict for LangGraph persistence or JSON export."""
        return {
            "element_id": self.element_id,
            "type": self.__class__.__name__,
            "metadata": self.metadata,
            "children": [c.to_dict() for c in self.children]
        }
    
    embedding: Optional[List[float]] = None
    
    def has_embedding(self) -> bool:
        return self.embedding is not None and len(self.embedding) > 0

@dataclass
class TextElement(BaseElement):
    content: str = ""
    def to_dict(self) -> Dict[str, Any]:
        d = super().to_dict()
        d["content"] = self.content
        return d

@dataclass
class ImageElement(BaseElement):
    content: Optional[Image.Image] = None  # Live object for local use
    base64_data: Optional[str] = None

@dataclass
class TableElement(BaseElement):
    content: str = "" # Usually Markdown string
============================================================


=== FILE: llm_src.py ===
============================================================
"""LLM + vision model factory via LangChain-Groq."""

from __future__ import annotations

import os
import logging
from functools import lru_cache

from langchain_groq import ChatGroq

# Setup module-level logger
logger = logging.getLogger(__name__)
from dotenv import load_dotenv
load_dotenv()

LLM_MODEL: str = os.getenv("LLM_MODEL", "openai/gpt-oss-120b")
VISION_MODEL: str = os.getenv("VISION_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
GROQ_API_KEY: str = os.getenv("GROQ_API_KEY", "")

@lru_cache(maxsize=1)
def get_llm() -> ChatGroq:
    """Return a cached ChatGroq LLM instance with config validation."""
    if not GROQ_API_KEY:
        logger.error("GROQ_API_KEY is missing from environment variables.")
        raise ValueError("GROQ_API_KEY not set.")

    logger.info(f"Initializing standard LLM | Model: {LLM_MODEL} | Temp: 0.0")
    
    return ChatGroq(
        model=LLM_MODEL,
        api_key=GROQ_API_KEY,
        temperature=0.0,
        max_tokens=2048,
    )

@lru_cache(maxsize=1)
def get_vision_llm() -> ChatGroq:
    """Return a cached ChatGroq vision-capable instance with config validation."""
    if not GROQ_API_KEY:
        logger.error("GROQ_API_KEY is missing from environment variables.")
        raise ValueError("GROQ_API_KEY not set.")

    logger.info(f"Initializing Vision LLM | Model: {VISION_MODEL} | Temp: 0.0")
    
    return ChatGroq(
        model=VISION_MODEL,
        api_key=GROQ_API_KEY,
        temperature=0.0,
        max_tokens=512,
    )

import io
import base64
from PIL import Image
from langchain_core.messages import HumanMessage

def call_vision_llm(
    image: Image.Image,
    prompt: str = "Describe this image in detail for retrieval in a RAG system. Focus on visible objects, text, layout, and context."
) -> str:
    """Vision convenience wrapper used by ImageDescriber."""
    llm = get_vision_llm()
    
    # Convert PIL → base64 for Groq/Llama-3.2-vision
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG")
    img_base64 = base64.b64encode(buffered.getvalue()).decode()

    message = HumanMessage(content=[
        {"type": "text", "text": prompt},
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"}
        }
    ])
    
    response = llm.invoke([message])
    return response.content.strip()
============================================================


=== FILE: databases.py ===
============================================================
import logging
import os
import time
from contextlib import asynccontextmanager
from typing import AsyncGenerator

from sqlalchemy.ext.asyncio import create_async_engine
from sqlmodel import SQLModel
from sqlmodel.ext.asyncio.session import AsyncSession as SQLModelSession
from sqlalchemy import text

# Setup module-level logger
logger = logging.getLogger(__name__)

DATABASE_URL: str = os.getenv(
    "DATABASE_URL",
    "postgresql+asyncpg://mrag:mrag_secret@localhost:5432/mrag",
)

# engine: Set echo=True only for local debugging to see raw SQL
engine = create_async_engine(DATABASE_URL, echo=False, future=True)

async def init_db() -> None:
    """Create all tables (idempotent)."""
    logger.info("Initializing database: Checking and creating tables...")
    try:
        async with engine.begin() as conn:
            await conn.run_sync(SQLModel.metadata.create_all)
        logger.info("Database initialization complete.")
    except Exception as e:
        logger.error(f"Database initialization FAILED: {e}")
        raise

@asynccontextmanager
async def get_session() -> AsyncGenerator[SQLModelSession, None]:
    """Yield an async SQLModel session with transaction logging."""
    start_time = time.perf_counter()
    session_id = id(start_time) # Simple unique ID for tracing the session
    
    logger.debug(f"[DB SESSION {session_id}] Opening session.")
    
    async with SQLModelSession(engine) as session:
        try:
            yield session
            await session.commit()
            duration = time.perf_counter() - start_time
            logger.debug(f"[DB SESSION {session_id}] Committed | Duration: {duration:.3f}s")
        except Exception as e:
            await session.rollback()
            duration = time.perf_counter() - start_time
            logger.error(f"[DB SESSION {session_id}] ROLLBACK due to error: {e} | Duration: {duration:.3f}s")
            raise
        finally:
            # SQLModelSession closes automatically via 'async with', 
            # but we log the end of the context
            pass

async def check_db_connection() -> None:
    """Verify Postgres connection on startup."""
    logger.info("Checking database connection...")
    try:
        async with engine.connect() as conn:
            await conn.execute(text("SELECT 1"))
        logger.info("Database connection verified successfully.")
    except Exception as e:
        logger.critical(f"FATAL: Database is unreachable at {DATABASE_URL}. Error: {e}")
        raise
============================================================


=== FILE: loaders/image.py ===
============================================================
# core/loaders/image.py
import io
import base64
from PIL import Image
from typing import List, Any
from ..elements import ImageElement, BaseElement
from .base import BaseLoader

class ImageLoader(BaseLoader):
    @staticmethod
    def to_base64(img: Image.Image) -> str:
        buffered = io.BytesIO()
        img.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()

    @staticmethod
    def process_raw_bytes(data: bytes, metadata: dict) -> ImageElement:
        img = Image.open(io.BytesIO(data)).convert("RGB")
        img.load()
        
        # Add dimensions to metadata automatically
        metadata.update({"width": img.width, "height": img.height})
        
        return ImageElement(
            content=img,
            base64_data=ImageLoader.to_base64(img),
            metadata=metadata
        )

    @staticmethod
    def load(file_path: str, **kwargs) -> List[BaseElement]:
        with open(file_path, "rb") as f:
            data = f.read()
        return [ImageLoader.process_raw_bytes(data, {"source": file_path})]
============================================================


=== FILE: loaders/__init__.py ===
============================================================
from .text import TextLoader
from .image import ImageLoader
from .pdf import PDFLoader

LOADER_MAPPING = {
    ".pdf": PDFLoader,
    ".txt": TextLoader,
    ".md": TextLoader,
    ".png": ImageLoader,
    ".jpg": ImageLoader,
    ".jpeg": ImageLoader,
}

def get_loader_for_file(file_path: str):
    import os
    ext = os.path.splitext(file_path.lower())[1]
    loader_cls = LOADER_MAPPING.get(ext)
    if not loader_cls:
        raise ValueError(f"No loader found for extension: {ext}")
    return loader_cls

__all__ = ["TextLoader", "ImageLoader", "PDFLoader", "get_loader_for_file"]
============================================================


=== FILE: loaders/pdf.py ===
============================================================
import fitz
import logging
from typing import List
from ..elements import BaseElement, TextElement, TableElement
from .base import BaseLoader
from .image import ImageLoader # Import the peer loader

logger = logging.getLogger(__name__)

class PDFLoader(BaseLoader):
    @staticmethod
    def load(file_path: str) -> List[BaseElement]:
        doc = fitz.open(file_path)
        elements: List[BaseElement] = []

        for page_num in range(len(doc)):
            page = doc[page_num] 
            curr = page_num + 1
            
            # 1. Text
            text = page.get_text("text").strip()
            if text:
                elements.append(TextElement(content=text, metadata={"page": curr}))

            # 2. Images - Using ImageLoader's logic
            for img_info in page.get_images(full=True):
                xref = img_info[0]
                base_img = doc.extract_image(xref)
                if base_img:
                    # Delegate processing to ImageLoader
                    img_el = ImageLoader.process_raw_bytes(
                        base_img["image"], 
                        {"page": curr, "xref": xref, "ext": base_img["ext"]}
                    )
                    elements.append(img_el)

            # # 3. Tables
            # try:
            #     for t in page.find_tables().tables:
            #         elements.append(TableElement(
            #             content=t.to_markdown(), 
            #             metadata={"page": curr}
            #         ))
            # except Exception:
            #     pass 

        doc.close()
        return elements
============================================================


=== FILE: loaders/base.py ===
============================================================
from abc import ABC, abstractmethod
from typing import List
from ..elements import BaseElement

class BaseLoader(ABC):
    @staticmethod
    @abstractmethod
    def load(file_path: str) -> List[BaseElement]:
        """Return flat list of elements. Pure. No side effects."""
        ...
============================================================


=== FILE: loaders/text.py ===
============================================================
from typing import List
from ..elements import TextElement, BaseElement
from .base import BaseLoader

class TextLoader(BaseLoader):
    @staticmethod
    def load(file_path: str) -> List[BaseElement]:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        return [TextElement(content=content, metadata={"source": file_path})]
============================================================


=== FILE: chunkers/__init__.py ===
============================================================
from .base import BaseChunker
from .recursive import RecursiveChunker
from .hierarchical import HierarchicalChunker
from .semantic import SemanticChunker

# A mapping for dynamic selection in your LangGraph nodes
CHUNKER_MAPPING = {
    "recursive": RecursiveChunker,
    "hierarchical": HierarchicalChunker,
    "semantic": SemanticChunker
}

def get_chunker(method: str = "recursive", **kwargs) -> BaseChunker:
    """Factory function to initialize a chunker with specific settings."""
    chunker_cls = CHUNKER_MAPPING.get(method.lower())
    if not chunker_cls:
        raise ValueError(f"Unknown chunking method: {method}")
    return chunker_cls(**kwargs)

__all__ = [
    "BaseChunker",
    "RecursiveChunker",
    "HierarchicalChunker",
    "SemanticChunker",
    "get_chunker"
]
============================================================


=== FILE: chunkers/hierarchical.py ===
============================================================
import logging
from typing import List
import os

from ..elements import BaseElement, TextElement
from .base import BaseChunker
from .recursive import RecursiveChunker  # reuse existing logic

logger = logging.getLogger(__name__)


class HierarchicalChunker(BaseChunker):
    """
    Hierarchical (Parent → Child) chunker.
    - Returns parent chunks at top level.
    - Each parent has .children (small chunks) → matches BaseChunker docstring.
    """

    def __init__(
        self,
        parent_chunk_size: int = 2000,
        child_chunk_size: int = 800,
        chunk_overlap: int = 100,
    ):
        self.parent_chunker = RecursiveChunker(
            chunk_size=parent_chunk_size, chunk_overlap=chunk_overlap
        )
        self.child_chunker = RecursiveChunker(
            chunk_size=child_chunk_size, chunk_overlap=chunk_overlap // 2
        )

        self.parent_chunk_size = parent_chunk_size
        self.child_chunk_size = child_chunk_size

    def chunk(self, elements: List[BaseElement]) -> List[BaseElement]:
        # 1. Create larger parent chunks
        parents = self.parent_chunker.chunk(elements)
        final_chunks: List[BaseElement] = []

        logger.debug(
            f"Starting hierarchical chunking: parent={self.parent_chunk_size}, "
            f"child={self.child_chunk_size}"
        )

        for parent in parents:
            if not isinstance(parent, TextElement):
                final_chunks.append(parent)
                continue

            # 2. Create child chunks from this parent only
            children = self.child_chunker.chunk([parent])

            # Attach children for hierarchy
            parent.children = []  # BaseElement / TextElement supports this (per docstring)
            for child in children:
                if isinstance(child, TextElement):
                    child.metadata.update(
                        {
                            "chunk_type": "child",
                            "parent_id": getattr(parent, "element_id", None),
                            "chunking_method": "hierarchical",
                        }
                    )
                    parent.children.append(child)

            # Parent gets its own metadata
            parent.metadata.update(
                {
                    "chunk_type": "parent",
                    "chunking_method": "hierarchical",
                }
            )

            final_chunks.append(parent)  # only parents go to top-level list

        logger.info(
            f"Hierarchical chunking complete: {len(parents)} parent chunks "
            f"(each with child chunks in .children)"
        )
        return final_chunks
============================================================


=== FILE: chunkers/recursive.py ===
============================================================
import logging
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from ..elements import BaseElement, TextElement
from .base import BaseChunker

# Get a logger specific to this module
logger = logging.getLogger(__name__)

class RecursiveChunker(BaseChunker):
    def __init__(self, chunk_size: int = 128, chunk_overlap: int = 32):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def chunk(self, elements: List[BaseElement]) -> List[BaseElement]:
        chunks: List[BaseElement] = []
        text_element_count = 0
        other_element_count = 0

        logger.debug(f"Starting chunking: size={self.chunk_size}, overlap={self.chunk_overlap}")

        for el in elements:
            if isinstance(el, TextElement):
                text_element_count += 1
                # Optional: log very large text elements that might cause bottlenecks
                if len(el.content) > 5000:
                    logger.debug(f"Splitting large TextElement (ID: {el.element_id}, Chars: {len(el.content)})")
                
                texts = self.splitter.split_text(el.content)
                
                for i, txt in enumerate(texts):
                    chunks.append(TextElement(
                        content=txt,
                        metadata={
                            **el.metadata, 
                            "chunk_index": i, 
                            "parent_id": el.element_id
                        }
                    ))
            else:
                # Non-text elements (Images/Tables) are passed through
                other_element_count += 1
                chunks.append(el)

        logger.info(
            f"Chunking complete: {text_element_count} text elements split into {len(chunks) - other_element_count} chunks. "
            f"{other_element_count} non-text elements preserved."
        )

        return chunks
============================================================


=== FILE: chunkers/semantic.py ===
============================================================
import logging
from typing import List, Optional
import numpy as np

from ..elements import BaseElement, TextElement
from ..embedders.text import TextEmbedder  # Local import
from .base import BaseChunker

logger = logging.getLogger(__name__)

class SemanticChunker(BaseChunker):
    """
    Semantic similarity-based chunker using local TextEmbedder.
    Splits text into sentences, embeds them, and breaks when similarity drops.
    """

    def __init__(
        self,
        embedder: Optional[TextEmbedder] = None,
        breakpoint_threshold: float = 0.8, # Adjust based on model (0.0 to 1.0)
    ):
        # Use local TextEmbedder logic
        self.embedder = embedder or TextEmbedder()
        self.breakpoint_threshold = breakpoint_threshold
        logger.info(f"SemanticChunker initialized with threshold {breakpoint_threshold}")

    def _split_sentences(self, text: str) -> List[str]:
        """Simple sentence splitter; can be swapped for nltk/spacy."""
        import re
        sentences = re.split(r'(?<=[.!?]) +', text)
        return [s.strip() for s in sentences if s.strip()]

    def chunk(self, elements: List[BaseElement]) -> List[BaseElement]:
        final_chunks: List[BaseElement] = []

        for el in elements:
            if not isinstance(el, TextElement) or not el.content.strip():
                final_chunks.append(el)
                continue

            sentences = self._split_sentences(el.content)
            if len(sentences) <= 1:
                final_chunks.append(el)
                continue

            # 1. Get embeddings for all sentences using your local logic
            embeddings = np.array(self.embedder.embed(sentences))
            
            # 2. Calculate cosine similarity between adjacent sentences
            # Formula: (A . B) / (||A|| * ||B||) - Note: TextEmbedder normalizes already
            clusters = []
            current_cluster = [sentences[0]]
            
            for i in range(len(sentences) - 1):
                similarity = np.dot(embeddings[i], embeddings[i+1])
                
                if similarity < self.breakpoint_threshold:
                    clusters.append(" ".join(current_cluster))
                    current_cluster = [sentences[i+1]]
                else:
                    current_cluster.append(sentences[i+1])
            
            clusters.append(" ".join(current_cluster))

            # 3. Create new TextElements
            for i, cluster_text in enumerate(clusters):
                final_chunks.append(
                    TextElement(
                        content=cluster_text,
                        metadata={
                            **el.metadata,
                            "chunk_index": i,
                            "parent_id": getattr(el, "element_id", None),
                            "chunking_method": "semantic_local",
                        },
                    )
                )

        logger.info(f"Semantic chunking complete. Generated {len(final_chunks)} chunks.")
        return final_chunks
============================================================


=== FILE: chunkers/base.py ===
============================================================
from abc import ABC, abstractmethod
from typing import List
from ..elements import BaseElement

class BaseChunker(ABC):
    @abstractmethod
    def chunk(self, elements: List[BaseElement]) -> List[BaseElement]:
        """Return list of chunk elements (may have .children for hierarchy)."""
        ...
============================================================


=== FILE: processors/image_describer.py ===
============================================================
import logging
from typing import List, Optional
from PIL import Image
import io
import base64
from ..elements import ImageElement, TextElement
# Assuming you have a standard LLM caller in core/llm_src.py
from ..llm_src import call_vision_llm 

logger = logging.getLogger(__name__)

class ImageDescriber:
    """Uses a Vision LLM to turn ImageElements into descriptive TextElements."""
    
    def __init__(self, prompt: str = "Describe this image in detail for a RAG search index."):
        self.prompt = prompt

    def describe(self, image_elements: List[ImageElement]) -> List[TextElement]:
        """Converts ImageElements to TextElements with the same metadata."""
        described_elements = []
        
        for img_el in image_elements:
            try:
                # 1. Get image (from PIL object or Base64)
                img_data = img_el.content
                
                # 2. Call Vision LLM (Logic resides in your llm_src.py)
                description = call_vision_llm(img_data, self.prompt)
                
                # 3. Create a new TextElement that "replaces" the ImageElement
                # We preserve the original ID and metadata for traceability
                text_el = TextElement(
                    element_id=img_el.element_id,
                    content=description,
                    metadata={
                        **img_el.metadata,
                        "original_type": "image",
                        "description_method": "vision_llm"
                    }
                )
                described_elements.append(text_el)
                
                logger.info(f"Successfully described image {img_el.element_id}")
            except Exception as e:
                logger.error(f"Failed to describe image {img_el.element_id}: {e}")
                
        return described_elements
============================================================


=== FILE: storers/base.py ===
============================================================
from abc import ABC, abstractmethod
from typing import List
from ..elements import BaseElement

class BaseStorer(ABC):
    @abstractmethod
    def store(self, elements: List[BaseElement], collection_name: str):
        """Upload elements to the vector database."""
        ...
============================================================


=== FILE: storers/qdrant.py ===
============================================================
import logging
from typing import List, Dict, Any
from qdrant_client import QdrantClient
from qdrant_client.http import models
from ..elements import BaseElement, TextElement, ImageElement
from .base import BaseStorer

logger = logging.getLogger(__name__)

class QdrantStorer(BaseStorer):
    def __init__(self, host: str = "localhost", port: int = 6333, api_key: str = None):
        self.client = QdrantClient(host=host, port=port, api_key=api_key)
        logger.info(f"Connected to Qdrant at {host}:{port}")

    def _ensure_collection(self, collection_name: str, vector_config: Dict[str, int]):
        if not self.client.collection_exists(collection_name):
            vectors_config = {
                name: models.VectorParams(size=dim, distance=models.Distance.COSINE)
                for name, dim in vector_config.items()
            }
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config=vectors_config
            )
            logger.info(f"Created fresh collection '{collection_name}'")

    def store(self, elements: List[BaseElement], collection_name: str):
        if not elements:
            return

        points = []
        vector_config = {}
        VECTOR_NAME = "vector"   # ← Unified name for multimodal CLIP (fixes the error)

        for el in elements:
            if not getattr(el, "embedding", None):
                continue

            if VECTOR_NAME not in vector_config:
                vector_config[VECTOR_NAME] = len(el.embedding)

            payload = {
                "type": type(el).__name__,
                "metadata": getattr(el, "metadata", {}),
            }
            if isinstance(el, TextElement):
                payload["content"] = el.content
            elif isinstance(el, ImageElement):
                payload["base64_data"] = getattr(el, "base64_data", None)

            points.append(models.PointStruct(
                id=el.element_id,
                vector={VECTOR_NAME: el.embedding},
                payload=payload
            ))

        self._ensure_collection(collection_name, vector_config)
        self.client.upsert(collection_name=collection_name, points=points)
        logger.info(f"✅ Successfully stored {len(points)} points in '{collection_name}'")
============================================================


=== FILE: utils/logging.py ===
============================================================
import time
import logging
import functools
from typing import Callable, Any

logger = logging.getLogger("ingestion_pipeline")

def log_node(func: Callable):
    @functools.wraps(func)
    def wrapper(state: Any, *args, **kwargs):
        node_name = func.__name__.replace("node_", "").upper()
        doc_id = state.get("document_id", "unknown")
        
        logger.info(f"[{node_name}] Starting | Doc: {doc_id}")
        start_time = time.perf_counter()
        
        try:
            result = func(state, *args, **kwargs)
            
            end_time = time.perf_counter()
            duration = end_time - start_time
            
            # Context-aware counts
            count = len(result.get("chunks", [])) or len(result.get("elements", []))
            logger.info(f"[{node_name}] Finished | Items: {count} | Time: {duration:.2f}s")
            
            return result
        except Exception as e:
            logger.error(f"[{node_name}] FAILED | Doc: {doc_id} | Error: {str(e)}")
            raise e
            
    return wrapper
============================================================


=== FILE: embedders/image.py ===
============================================================
import logging
from typing import List
from PIL import Image
import torch
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)


class ImageEmbedder:
    """CLIP image-only embedder."""

    def __init__(self, model_name: str = "clip-ViT-B-32"):
        self._model_name = model_name
        logger.info(f"Loading CLIP image embedder: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = self.model.to(self.device)

    def embed(self, images: List[Image.Image]) -> List[List[float]]:
        """Batch embed PIL Images."""
        if not images:
            return []
        logger.debug(f"Embedding {len(images)} images with {self.model_name}...")
        embeddings = self.model.encode(
            images,
            batch_size=16,
            show_progress_bar=False,
            normalize_embeddings=True,
            convert_to_numpy=True,
        )
        return embeddings.tolist()
    
    @property
    def dimension(self) -> int:
        return self.model.get_sentence_embedding_dimension()

    @property
    def model_name(self) -> str:
        return self._model_name
============================================================


=== FILE: embedders/__init__.py ===
============================================================
from .base import BaseEmbedder
from .text import TextEmbedder
from .image import ImageEmbedder
from .multimodal import MultimodalEmbedder

# Registry for LangGraph nodes to select the embedding engine
EMBEDDER_MAPPING = {
    "text": TextEmbedder,
    "image": ImageEmbedder,
    "multimodal": MultimodalEmbedder
}

def get_embedder(type: str = "text", **kwargs) -> BaseEmbedder:
    """Factory to initialize the requested embedder."""
    embedder_cls = EMBEDDER_MAPPING.get(type.lower())
    if not embedder_cls:
        raise ValueError(f"Unknown embedder type: {type}. Choose from {list(EMBEDDER_MAPPING.keys())}")
    return embedder_cls(**kwargs)

__all__ = ["BaseEmbedder", "TextEmbedder", "ImageEmbedder", "MultimodalEmbedder", "get_embedder"]
============================================================


=== FILE: embedders/multimodal.py ===
============================================================
import logging
from typing import List, Union
from PIL import Image
import torch
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)


class MultimodalEmbedder:
    """Unified CLIP embedder – text AND images share the exact same vector space."""

    def __init__(self, model_name: str = "clip-ViT-B-32"):
        self._model_name = model_name
        logger.info(f"Loading unified multimodal CLIP: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = self.model.to(self.device)

    def embed(self, items: List[Union[str, Image.Image]]) -> List[List[float]]:
        """Accepts mixed list: str (text) or PIL.Image (image)."""
        if not items:
            return []

        logger.debug(f"Embedding {len(items)} multimodal items with {self.model_name}...")
        embeddings = self.model.encode(
            items,
            batch_size=16,
            show_progress_bar=False,
            normalize_embeddings=True,
            convert_to_numpy=True,
        )
        return embeddings.tolist()
    
    @property
    def dimension(self) -> int:
        return self.model.get_sentence_embedding_dimension()

    @property
    def model_name(self) -> str:
        return self._model_name
============================================================


=== FILE: embedders/base.py ===
============================================================
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseEmbedder(ABC):
    @abstractmethod
    def embed(self, items: List[Any]) -> List[List[float]]:
        """items = list of str (text) or list of PIL.Image (images)"""
        ...

    @property
    @abstractmethod
    def dimension(self) -> int:
        ...

    @property
    @abstractmethod
    def model_name(self) -> str:
        ...
============================================================


=== FILE: embedders/text.py ===
============================================================
import logging
from typing import List
from sentence_transformers import SentenceTransformer
from .base import BaseEmbedder

logger = logging.getLogger(__name__)


class TextEmbedder(BaseEmbedder):
    """Text-only embedder using SentenceTransformer (all-MiniLM, etc.)."""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self._model_name = model_name
        logger.info(f"Loading text embedder: {model_name}")
        self.model = SentenceTransformer(model_name)

    def embed(self, texts: List[str]) -> List[List[float]]:
        """Batch embed list of texts."""
        if not texts:
            return []
        logger.debug(f"Embedding {len(texts)} texts with {self.model_name}...")
        embeddings = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=False,
            normalize_embeddings=True,
            convert_to_numpy=True,
        )
        return embeddings.tolist()
    
    @property
    def dimension(self) -> int:
        return self.model.get_sentence_embedding_dimension()

    @property
    def model_name(self) -> str:
        return self._model_name # Ensure this is set in __init__
============================================================

============================================================


=== FILE: core/__init__.py ===
============================================================
from .state import IngestionState
from .elements import (
    BaseElement, TextElement, ImageElement,
    TableElement
)

__all__ = [
    "IngestionState",
    "BaseElement", "TextElement", "ImageElement",
    "TableElement",
]

============================================================


=== FILE: core/config.py ===
============================================================
"""
core/config.py
==============
Single source of truth for all configurable values.

Priority (highest → lowest):
  1. Per-request PipelineConfig payload (from frontend / API)
  2. Environment variables / .env file  (Settings)
  3. Hardcoded defaults below
"""
from __future__ import annotations
from typing import Literal, Optional
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings, SettingsConfigDict


# ---------------------------------------------------------------------------
# Infrastructure secrets — NEVER sent to the frontend
# ---------------------------------------------------------------------------
class Settings(BaseSettings):
    """Loaded once at startup from environment / .env file."""

    # ── Auth ──────────────────────────────────────────────────────────────
    groq_api_key: str = Field(..., alias="GROQ_API_KEY")

    # ── Qdrant ────────────────────────────────────────────────────────────
    qdrant_host: str = Field("localhost", alias="QDRANT_HOST")
    qdrant_port: int = Field(6333, alias="QDRANT_PORT")
    qdrant_api_key: Optional[str] = Field(None, alias="QDRANT_API_KEY")

    # ── Database ──────────────────────────────────────────────────────────
    database_url: str = Field(
        "postgresql+asyncpg://mrag:mrag_secret@localhost:5432/mrag",
        alias="DATABASE_URL",
    )

    # ── Default model names (overridable per-request) ─────────────────────
    default_llm_model: str = Field("llama3-70b-8192", alias="LLM_MODEL")
    default_vision_model: str = Field(
        "meta-llama/llama-4-scout-17b-16e-instruct", alias="VISION_MODEL"
    )
    default_embedding_model: str = Field(
        "all-MiniLM-L6-v2", alias="DEFAULT_EMBEDDING_MODEL"
    )
    default_embedding_type: Literal["text", "image", "multimodal"] = Field(
        "text", alias="DEFAULT_EMBEDDING_TYPE"
    )
    default_collection: str = Field("mrag_default", alias="DEFAULT_COLLECTION")

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        populate_by_name=True,
        extra="ignore",
    )


# ---------------------------------------------------------------------------
# Per-request pipeline configuration — safe to expose to the frontend
# ---------------------------------------------------------------------------
class PipelineConfig(BaseModel):
    """
    Everything the caller can tune per ingestion job.
    Validated by FastAPI automatically when sent as JSON.
    """

    # ── Image description ─────────────────────────────────────────────────
    describe_images: bool = True
    image_description_prompt: str = (
        "Describe this image in rich detail for multimodal RAG retrieval. "
        "Include all visible text, objects, charts, layout, and context."
    )

    # ── Chunking ──────────────────────────────────────────────────────────
    chunking_method: Literal["recursive", "hierarchical", "semantic"] = "recursive"
    chunk_size: int = Field(1000, ge=100, le=8000, description="Tokens per chunk (recursive)")
    chunk_overlap: int = Field(100, ge=0, le=500)
    parent_chunk_size: int = Field(2000, ge=500, le=16000, description="Parent size (hierarchical)")
    child_chunk_size: int = Field(800, ge=100, le=4000, description="Child size (hierarchical)")
    semantic_breakpoint: float = Field(0.8, ge=0.0, le=1.0, description="Cosine sim threshold (semantic)")

    # ── Embedding ─────────────────────────────────────────────────────────
    embedding_type: Literal["text", "image", "multimodal"] = "text"
    embedding_model: str = Field(
        "all-MiniLM-L6-v2",
        description="SentenceTransformer model name (e.g. all-MiniLM-L6-v2, clip-ViT-B-32)",
    )

    # ── Storage ───────────────────────────────────────────────────────────
    collection_name: str = Field("mrag_default", description="Qdrant collection")

    # ── LLM models ────────────────────────────────────────────────────────
    llm_model: str = "llama3-70b-8192"
    vision_model: str = "meta-llama/llama-4-scout-17b-16e-instruct"

    @classmethod
    def from_settings(cls, settings: Settings) -> "PipelineConfig":
        """Build a default config seeded from env/Settings."""
        return cls(
            embedding_type=settings.default_embedding_type,
            embedding_model=settings.default_embedding_model,
            collection_name=settings.default_collection,
            llm_model=settings.default_llm_model,
            vision_model=settings.default_vision_model,
        )
============================================================


=== FILE: core/state.py ===
============================================================
"""
core/state.py
=============
LangGraph state for the M-RAG ingestion pipeline.
"""
from __future__ import annotations
from typing import Annotated, Any, Dict, List, Optional
from typing_extensions import TypedDict

from .elements import BaseElement

def _replace(left: List[BaseElement], right: Optional[List[BaseElement]]) -> List[BaseElement]:
    """
    Return `right` when provided, otherwise keep `left`.
    This gives REPLACE semantics — the node fully owns the elements list it returns.
    """
    if right is None:
        return left
    return right


class IngestionState(TypedDict, total=False):
    """
    State object passed between LangGraph nodes.

    `total=False` → every key is optional so nodes only need to return
    the keys they actually mutate.
    """
    # Required on entry
    file_path: str
    document_id: str

    # Core payload — uses REPLACE reducer so nodes can swap the whole list
    elements: Annotated[List[BaseElement], _replace]

    # Per-run configuration (PipelineConfig serialised to dict for JSON-safety)
    config: Dict[str, Any]

    # Accumulated timing / count metrics (merged manually in each node)
    metrics: Dict[str, Any]

    # Set by node_store; one of "success" | "failed"
    status: str
    error: Optional[str]
============================================================


=== FILE: core/elements.py ===
============================================================
# import logging
# from pydantic import BaseModel, Field
# from typing import List, Dict, Any, Literal, Union, Optional
# from PIL import Image
# import uuid

# # Get logger for domain models
# logger = logging.getLogger(__name__)

# ElementType = Literal["text", "image", "table", "audio"]

# class BaseElement(BaseModel):
#     element_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
#     type: ElementType
#     content: Any
#     metadata: Dict[str, Any] = Field(default_factory=dict)
#     children: List["BaseElement"] = Field(default_factory=list)

#     embedding: Optional[List[float]] = None
#     embedding_model: Optional[str] = None

#     def get_text_for_retrieval(self) -> str:
#         """Unified text representation logic with logging for debugging retrieval paths."""
        
#         # 1. Check for processed multimodal descriptions
#         if isinstance(self, ImageElement) and self.description:
#             logger.debug(f"Retrieval: Using description for Image {self.element_id}")
#             return self.description
            
#         if isinstance(self, AudioElement) and self.transcription:
#             logger.debug(f"Retrieval: Using transcription for Audio {self.element_id}")
#             return self.transcription
            
#         # 2. Check for raw text content
#         if isinstance(self.content, str):
#             # We don't log here to avoid flooding, as this is the standard path
#             return self.content
            
#         # 3. Fallback path (potential quality issue)
#         logger.warning(
#             f"Retrieval fallback: Element {self.element_id} (type: {self.type}) "
#             f"has no specialized text. Using str() conversion."
#         )
#         return str(self.content)

#     class Config:
#         arbitrary_types_allowed = True 

# # --- Subclasses (Keep these as pure schemas) ---

# class TextElement(BaseElement):
#     type: ElementType = "text"
#     content: str

# class ImageElement(BaseElement):
#     type: ElementType = "image"
#     content: Image.Image
#     description: str = ""

# class TableElement(BaseElement):
#     type: ElementType = "table"
#     content: str # markdown

# class AudioElement(BaseElement):
#     type: ElementType = "audio"
#     content: str
#     transcription: str = ""


from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
from uuid import uuid4
from PIL import Image

@dataclass
class BaseElement:
    element_id: str = field(default_factory=lambda: str(uuid4()))
    content: Any = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    children: List["BaseElement"] = field(default_factory=list) # For Hierarchical chunking

    def to_dict(self) -> Dict[str, Any]:
        """Convert element to dict for LangGraph persistence or JSON export."""
        return {
            "element_id": self.element_id,
            "type": self.__class__.__name__,
            "metadata": self.metadata,
            "children": [c.to_dict() for c in self.children]
        }
    
    embedding: Optional[List[float]] = None
    
    def has_embedding(self) -> bool:
        return self.embedding is not None and len(self.embedding) > 0

@dataclass
class TextElement(BaseElement):
    content: str = ""
    def to_dict(self) -> Dict[str, Any]:
        d = super().to_dict()
        d["content"] = self.content
        return d

@dataclass
class ImageElement(BaseElement):
    content: Optional[Image.Image] = None  # Live object for local use
    base64_data: Optional[str] = None

@dataclass
class TableElement(BaseElement):
    content: str = "" # Usually Markdown string
============================================================


=== FILE: core/settings.py ===
============================================================
"""
core/settings.py
================
Cached Settings singleton — import get_settings() anywhere.
"""
from __future__ import annotations
from functools import lru_cache
from .config import Settings


@lru_cache(maxsize=1)
def get_settings() -> Settings:
    return Settings()
============================================================


=== FILE: core/llm_src.py ===
============================================================
"""LLM + vision model factory via LangChain-Groq."""

from __future__ import annotations

import os
import logging
from functools import lru_cache

from langchain_groq import ChatGroq

# Setup module-level logger
logger = logging.getLogger(__name__)
from dotenv import load_dotenv
load_dotenv()

LLM_MODEL: str = os.getenv("LLM_MODEL", "openai/gpt-oss-120b")
VISION_MODEL: str = os.getenv("VISION_MODEL", "meta-llama/llama-4-scout-17b-16e-instruct")
GROQ_API_KEY: str = os.getenv("GROQ_API_KEY", "")

@lru_cache(maxsize=1)
def get_llm() -> ChatGroq:
    """Return a cached ChatGroq LLM instance with config validation."""
    if not GROQ_API_KEY:
        logger.error("GROQ_API_KEY is missing from environment variables.")
        raise ValueError("GROQ_API_KEY not set.")

    logger.info(f"Initializing standard LLM | Model: {LLM_MODEL} | Temp: 0.0")
    
    return ChatGroq(
        model=LLM_MODEL,
        api_key=GROQ_API_KEY,
        temperature=0.0,
        max_tokens=2048,
    )

@lru_cache(maxsize=1)
def get_vision_llm() -> ChatGroq:
    """Return a cached ChatGroq vision-capable instance with config validation."""
    if not GROQ_API_KEY:
        logger.error("GROQ_API_KEY is missing from environment variables.")
        raise ValueError("GROQ_API_KEY not set.")

    logger.info(f"Initializing Vision LLM | Model: {VISION_MODEL} | Temp: 0.0")
    
    return ChatGroq(
        model=VISION_MODEL,
        api_key=GROQ_API_KEY,
        temperature=0.0,
        max_tokens=512,
    )

import io
import base64
from PIL import Image
from langchain_core.messages import HumanMessage

def call_vision_llm(
    image: Image.Image,
    prompt: str = "Describe this image in detail for retrieval in a RAG system. Focus on visible objects, text, layout, and context."
) -> str:
    """Vision convenience wrapper used by ImageDescriber."""
    llm = get_vision_llm()
    
    # Convert PIL → base64 for Groq/Llama-3.2-vision
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG")
    img_base64 = base64.b64encode(buffered.getvalue()).decode()

    message = HumanMessage(content=[
        {"type": "text", "text": prompt},
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"}
        }
    ])
    
    response = llm.invoke([message])
    return response.content.strip()
============================================================


=== FILE: core/databases.py ===
============================================================
import logging
import os
import time
from contextlib import asynccontextmanager
from typing import AsyncGenerator

from sqlalchemy.ext.asyncio import create_async_engine
from sqlmodel import SQLModel
from sqlmodel.ext.asyncio.session import AsyncSession as SQLModelSession
from sqlalchemy import text

# Setup module-level logger
logger = logging.getLogger(__name__)

DATABASE_URL: str = os.getenv(
    "DATABASE_URL",
    "postgresql+asyncpg://mrag:mrag_secret@localhost:5432/mrag",
)

# engine: Set echo=True only for local debugging to see raw SQL
engine = create_async_engine(DATABASE_URL, echo=False, future=True)

async def init_db() -> None:
    """Create all tables (idempotent)."""
    logger.info("Initializing database: Checking and creating tables...")
    try:
        async with engine.begin() as conn:
            await conn.run_sync(SQLModel.metadata.create_all)
        logger.info("Database initialization complete.")
    except Exception as e:
        logger.error(f"Database initialization FAILED: {e}")
        raise

@asynccontextmanager
async def get_session() -> AsyncGenerator[SQLModelSession, None]:
    """Yield an async SQLModel session with transaction logging."""
    start_time = time.perf_counter()
    session_id = id(start_time) # Simple unique ID for tracing the session
    
    logger.debug(f"[DB SESSION {session_id}] Opening session.")
    
    async with SQLModelSession(engine) as session:
        try:
            yield session
            await session.commit()
            duration = time.perf_counter() - start_time
            logger.debug(f"[DB SESSION {session_id}] Committed | Duration: {duration:.3f}s")
        except Exception as e:
            await session.rollback()
            duration = time.perf_counter() - start_time
            logger.error(f"[DB SESSION {session_id}] ROLLBACK due to error: {e} | Duration: {duration:.3f}s")
            raise
        finally:
            # SQLModelSession closes automatically via 'async with', 
            # but we log the end of the context
            pass

async def check_db_connection() -> None:
    """Verify Postgres connection on startup."""
    logger.info("Checking database connection...")
    try:
        async with engine.connect() as conn:
            await conn.execute(text("SELECT 1"))
        logger.info("Database connection verified successfully.")
    except Exception as e:
        logger.critical(f"FATAL: Database is unreachable at {DATABASE_URL}. Error: {e}")
        raise
============================================================


=== FILE: core/loaders/image.py ===
============================================================
# core/loaders/image.py
import io
import base64
from PIL import Image
from typing import List, Any
from ..elements import ImageElement, BaseElement
from .base import BaseLoader

class ImageLoader(BaseLoader):
    @staticmethod
    def to_base64(img: Image.Image) -> str:
        buffered = io.BytesIO()
        img.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()

    @staticmethod
    def process_raw_bytes(data: bytes, metadata: dict) -> ImageElement:
        img = Image.open(io.BytesIO(data)).convert("RGB")
        img.load()
        
        # Add dimensions to metadata automatically
        metadata.update({"width": img.width, "height": img.height})
        
        return ImageElement(
            content=img,
            base64_data=ImageLoader.to_base64(img),
            metadata=metadata
        )

    @staticmethod
    def load(file_path: str, **kwargs) -> List[BaseElement]:
        with open(file_path, "rb") as f:
            data = f.read()
        return [ImageLoader.process_raw_bytes(data, {"source": file_path})]
============================================================


=== FILE: core/loaders/__init__.py ===
============================================================
from .text import TextLoader
from .image import ImageLoader
from .pdf import PDFLoader

LOADER_MAPPING = {
    ".pdf": PDFLoader,
    ".txt": TextLoader,
    ".md": TextLoader,
    ".png": ImageLoader,
    ".jpg": ImageLoader,
    ".jpeg": ImageLoader,
}

def get_loader_for_file(file_path: str):
    import os
    ext = os.path.splitext(file_path.lower())[1]
    loader_cls = LOADER_MAPPING.get(ext)
    if not loader_cls:
        raise ValueError(f"No loader found for extension: {ext}")
    return loader_cls

__all__ = ["TextLoader", "ImageLoader", "PDFLoader", "get_loader_for_file"]
============================================================


=== FILE: core/loaders/pdf.py ===
============================================================
import fitz
import logging
from typing import List
from ..elements import BaseElement, TextElement, TableElement
from .base import BaseLoader
from .image import ImageLoader # Import the peer loader

logger = logging.getLogger(__name__)

class PDFLoader(BaseLoader):
    @staticmethod
    def load(file_path: str) -> List[BaseElement]:
        doc = fitz.open(file_path)
        elements: List[BaseElement] = []

        for page_num in range(len(doc)):
            page = doc[page_num] 
            curr = page_num + 1
            
            # 1. Text
            text = page.get_text("text").strip()
            if text:
                elements.append(TextElement(content=text, metadata={"page": curr}))

            # 2. Images - Using ImageLoader's logic
            for img_info in page.get_images(full=True):
                xref = img_info[0]
                base_img = doc.extract_image(xref)
                if base_img:
                    # Delegate processing to ImageLoader
                    img_el = ImageLoader.process_raw_bytes(
                        base_img["image"], 
                        {"page": curr, "xref": xref, "ext": base_img["ext"]}
                    )
                    elements.append(img_el)

            # # 3. Tables
            # try:
            #     for t in page.find_tables().tables:
            #         elements.append(TableElement(
            #             content=t.to_markdown(), 
            #             metadata={"page": curr}
            #         ))
            # except Exception:
            #     pass 

        doc.close()
        return elements
============================================================


=== FILE: core/loaders/base.py ===
============================================================
from abc import ABC, abstractmethod
from typing import List
from ..elements import BaseElement

class BaseLoader(ABC):
    @staticmethod
    @abstractmethod
    def load(file_path: str) -> List[BaseElement]:
        """Return flat list of elements. Pure. No side effects."""
        ...
============================================================


=== FILE: core/loaders/text.py ===
============================================================
from typing import List
from ..elements import TextElement, BaseElement
from .base import BaseLoader

class TextLoader(BaseLoader):
    @staticmethod
    def load(file_path: str) -> List[BaseElement]:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        return [TextElement(content=content, metadata={"source": file_path})]
============================================================


=== FILE: core/chunkers/__init__.py ===
============================================================
from .base import BaseChunker
from .recursive import RecursiveChunker
from .hierarchical import HierarchicalChunker
from .semantic import SemanticChunker

# A mapping for dynamic selection in your LangGraph nodes
CHUNKER_MAPPING = {
    "recursive": RecursiveChunker,
    "hierarchical": HierarchicalChunker,
    "semantic": SemanticChunker
}

def get_chunker(method: str = "recursive", **kwargs) -> BaseChunker:
    """Factory function to initialize a chunker with specific settings."""
    chunker_cls = CHUNKER_MAPPING.get(method.lower())
    if not chunker_cls:
        raise ValueError(f"Unknown chunking method: {method}")
    return chunker_cls(**kwargs)

__all__ = [
    "BaseChunker",
    "RecursiveChunker",
    "HierarchicalChunker",
    "SemanticChunker",
    "get_chunker"
]
============================================================


=== FILE: core/chunkers/hierarchical.py ===
============================================================
"""
core/chunkers/hierarchical.py
==============================
Hierarchical (Parent → Child) chunker.

Fix applied:
  Children are tagged so node_chunk can flatten them into a single list
  before embedding.  Every parent AND every child gets embedded and stored —
  parents give broad context for retrieval, children give precision.
"""
from __future__ import annotations

import logging
from typing import List

from ..elements import BaseElement, TextElement
from .base import BaseChunker
from .recursive import RecursiveChunker

logger = logging.getLogger(__name__)


class HierarchicalChunker(BaseChunker):
    def __init__(
        self,
        parent_chunk_size: int = 2000,
        child_chunk_size: int = 800,
        chunk_overlap: int = 100,
    ):
        self.parent_chunker = RecursiveChunker(
            chunk_size=parent_chunk_size, chunk_overlap=chunk_overlap
        )
        self.child_chunker = RecursiveChunker(
            chunk_size=child_chunk_size, chunk_overlap=chunk_overlap // 2
        )
        self.parent_chunk_size = parent_chunk_size
        self.child_chunk_size = child_chunk_size

    def chunk(self, elements: List[BaseElement]) -> List[BaseElement]:
        parents = self.parent_chunker.chunk(elements)
        final_chunks: List[BaseElement] = []

        logger.debug(
            f"Hierarchical chunking: parent_size={self.parent_chunk_size}, "
            f"child_size={self.child_chunk_size}"
        )

        for parent in parents:
            if not isinstance(parent, TextElement):
                final_chunks.append(parent)
                continue

            parent_id = parent.element_id
            children = self.child_chunker.chunk([parent])

            tagged_children: List[TextElement] = []
            for idx, child in enumerate(children):
                if isinstance(child, TextElement):
                    child.metadata.update(
                        {
                            "chunk_type": "child",
                            "parent_id": parent_id,
                            "chunking_method": "hierarchical",
                            "child_index": idx,
                        }
                    )
                    tagged_children.append(child)

            parent.metadata.update(
                {
                    "chunk_type": "parent",
                    "chunking_method": "hierarchical",
                    "child_count": len(tagged_children),
                }
            )
            # Store children on parent — node_chunk._flatten_elements() will
            # expand these into the flat embedding list.
            parent.children = tagged_children
            final_chunks.append(parent)

        logger.info(
            f"Hierarchical chunking complete: {len(parents)} parents, "
            f"{sum(len(p.children) for p in final_chunks if hasattr(p, 'children'))} children"
        )
        return final_chunks
============================================================


=== FILE: core/chunkers/recursive.py ===
============================================================
import logging
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from ..elements import BaseElement, TextElement
from .base import BaseChunker

# Get a logger specific to this module
logger = logging.getLogger(__name__)

class RecursiveChunker(BaseChunker):
    def __init__(self, chunk_size: int = 128, chunk_overlap: int = 32):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def chunk(self, elements: List[BaseElement]) -> List[BaseElement]:
        chunks: List[BaseElement] = []
        text_element_count = 0
        other_element_count = 0

        logger.debug(f"Starting chunking: size={self.chunk_size}, overlap={self.chunk_overlap}")

        for el in elements:
            if isinstance(el, TextElement):
                text_element_count += 1
                # Optional: log very large text elements that might cause bottlenecks
                if len(el.content) > 5000:
                    logger.debug(f"Splitting large TextElement (ID: {el.element_id}, Chars: {len(el.content)})")
                
                texts = self.splitter.split_text(el.content)
                
                for i, txt in enumerate(texts):
                    chunks.append(TextElement(
                        content=txt,
                        metadata={
                            **el.metadata, 
                            "chunk_index": i, 
                            "parent_id": el.element_id
                        }
                    ))
            else:
                # Non-text elements (Images/Tables) are passed through
                other_element_count += 1
                chunks.append(el)

        logger.info(
            f"Chunking complete: {text_element_count} text elements split into {len(chunks) - other_element_count} chunks. "
            f"{other_element_count} non-text elements preserved."
        )

        return chunks
============================================================


=== FILE: core/chunkers/semantic.py ===
============================================================
import logging
from typing import List, Optional
import numpy as np

from ..elements import BaseElement, TextElement
from ..embedders.text import TextEmbedder  # Local import
from .base import BaseChunker

logger = logging.getLogger(__name__)

class SemanticChunker(BaseChunker):
    """
    Semantic similarity-based chunker using local TextEmbedder.
    Splits text into sentences, embeds them, and breaks when similarity drops.
    """

    def __init__(
        self,
        embedder: Optional[TextEmbedder] = None,
        breakpoint_threshold: float = 0.8, # Adjust based on model (0.0 to 1.0)
    ):
        # Use local TextEmbedder logic
        self.embedder = embedder or TextEmbedder()
        self.breakpoint_threshold = breakpoint_threshold
        logger.info(f"SemanticChunker initialized with threshold {breakpoint_threshold}")

    def _split_sentences(self, text: str) -> List[str]:
        """Simple sentence splitter; can be swapped for nltk/spacy."""
        import re
        sentences = re.split(r'(?<=[.!?]) +', text)
        return [s.strip() for s in sentences if s.strip()]

    def chunk(self, elements: List[BaseElement]) -> List[BaseElement]:
        final_chunks: List[BaseElement] = []

        for el in elements:
            if not isinstance(el, TextElement) or not el.content.strip():
                final_chunks.append(el)
                continue

            sentences = self._split_sentences(el.content)
            if len(sentences) <= 1:
                final_chunks.append(el)
                continue

            # 1. Get embeddings for all sentences using your local logic
            embeddings = np.array(self.embedder.embed(sentences))
            
            # 2. Calculate cosine similarity between adjacent sentences
            # Formula: (A . B) / (||A|| * ||B||) - Note: TextEmbedder normalizes already
            clusters = []
            current_cluster = [sentences[0]]
            
            for i in range(len(sentences) - 1):
                similarity = np.dot(embeddings[i], embeddings[i+1])
                
                if similarity < self.breakpoint_threshold:
                    clusters.append(" ".join(current_cluster))
                    current_cluster = [sentences[i+1]]
                else:
                    current_cluster.append(sentences[i+1])
            
            clusters.append(" ".join(current_cluster))

            # 3. Create new TextElements
            for i, cluster_text in enumerate(clusters):
                final_chunks.append(
                    TextElement(
                        content=cluster_text,
                        metadata={
                            **el.metadata,
                            "chunk_index": i,
                            "parent_id": getattr(el, "element_id", None),
                            "chunking_method": "semantic_local",
                        },
                    )
                )

        logger.info(f"Semantic chunking complete. Generated {len(final_chunks)} chunks.")
        return final_chunks
============================================================


=== FILE: core/chunkers/base.py ===
============================================================
from abc import ABC, abstractmethod
from typing import List
from ..elements import BaseElement

class BaseChunker(ABC):
    @abstractmethod
    def chunk(self, elements: List[BaseElement]) -> List[BaseElement]:
        """Return list of chunk elements (may have .children for hierarchy)."""
        ...
============================================================


=== FILE: core/processors/image_describer.py ===
============================================================
"""
core/processors/image_describer.py
====================================
Vision-LLM image description node.

Fix: vision_model is accepted as a constructor argument so it can be
overridden per-request via PipelineConfig.
"""
from __future__ import annotations

import logging
import os
from typing import List, Optional

from PIL import Image

from ..elements import ImageElement, TextElement

logger = logging.getLogger(__name__)

_DEFAULT_PROMPT = (
    "Describe this image in rich detail for multimodal RAG retrieval. "
    "Include all visible text, objects, charts, layout, and context."
)


class ImageDescriber:
    """Converts ImageElements → TextElements using a Vision LLM."""

    def __init__(
        self,
        prompt: Optional[str] = None,
        vision_model: Optional[str] = None,
    ):
        self.prompt = prompt or _DEFAULT_PROMPT
        # Override model globally so get_vision_llm() picks it up
        if vision_model:
            os.environ["VISION_MODEL"] = vision_model

    def describe(self, image_elements: List[ImageElement]) -> List[TextElement]:
        from ..llm_src import call_vision_llm   # local import keeps module lightweight

        described: List[TextElement] = []

        for img_el in image_elements:
            try:
                img_data: Optional[Image.Image] = img_el.content
                if img_data is None:
                    logger.warning(
                        f"ImageElement {img_el.element_id} has no PIL content — skipping."
                    )
                    continue

                description = call_vision_llm(img_data, self.prompt)

                text_el = TextElement(
                    element_id=img_el.element_id,   # preserve ID for safe dedup
                    content=description,
                    metadata={
                        **img_el.metadata,
                        "original_type": "image",
                        "description_method": "vision_llm",
                    },
                )
                described.append(text_el)
                logger.info(f"Described image {img_el.element_id} ({len(description)} chars)")

            except Exception as exc:
                logger.error(f"Failed to describe image {img_el.element_id}: {exc}")

        return described
============================================================


=== FILE: core/storers/base.py ===
============================================================
from abc import ABC, abstractmethod
from typing import List
from ..elements import BaseElement

class BaseStorer(ABC):
    @abstractmethod
    def store(self, elements: List[BaseElement], collection_name: str):
        """Upload elements to the vector database."""
        ...
============================================================


=== FILE: core/storers/qdrant.py ===
============================================================
"""
core/storers/qdrant.py
======================
Qdrant storer with dynamic collection schema.

Fix applied:
  Collection schema (vector dimension) is derived from the actual embeddings
  on the elements at store-time rather than being hardcoded.  This means
  swapping from all-MiniLM (384-d) to clip-ViT-B-32 (512-d) in config
  just works — the correct collection is created automatically.

  Also returns stored_count so node_store can log it.
"""
from __future__ import annotations

import logging
from typing import Dict, List, Optional

from qdrant_client import QdrantClient
from qdrant_client.http import models

from ..elements import BaseElement, ImageElement, TextElement
from .base import BaseStorer

logger = logging.getLogger(__name__)

VECTOR_NAME = "default"   # single named vector per point


class QdrantStorer(BaseStorer):
    def __init__(
        self,
        host: str = "localhost",
        port: int = 6333,
        api_key: Optional[str] = None,
    ):
        self.client = QdrantClient(host=host, port=port, api_key=api_key,https=False)
        logger.info(f"Connected to Qdrant at {host}:{port}")

    # ------------------------------------------------------------------
    def _ensure_collection(self, collection_name: str, vector_dim: int) -> None:
        """
        Create the collection if it doesn't exist.
        If it does exist, verify dimension compatibility and warn on mismatch.
        """
        if not self.client.collection_exists(collection_name):
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config={
                    VECTOR_NAME: models.VectorParams(
                        size=vector_dim,
                        distance=models.Distance.COSINE,
                    )
                },
            )
            logger.info(
                f"Created collection '{collection_name}' with dim={vector_dim}"
            )
        else:
            # Validate existing dimension matches
            info = self.client.get_collection(collection_name)
            existing_dim = (
                info.config.params.vectors[VECTOR_NAME].size
                if isinstance(info.config.params.vectors, dict)
                else info.config.params.vectors.size
            )
            if existing_dim != vector_dim:
                logger.warning(
                    f"Collection '{collection_name}' exists with dim={existing_dim} "
                    f"but current embedder produces dim={vector_dim}. "
                    "Consider using a different collection_name to avoid mixed embeddings."
                )

    # ------------------------------------------------------------------
    def store(self, elements: List[BaseElement], collection_name: str) -> int:
        """
        Upload elements to Qdrant.  Returns the number of points stored.
        """
        if not elements:
            return 0

        # ── Derive vector dimension from first embedded element ───────
        embedded = [el for el in elements if getattr(el, "embedding", None)]
        if not embedded:
            logger.warning("No embedded elements to store — skipping.")
            return 0

        vector_dim: int = len(embedded[0].embedding)
        self._ensure_collection(collection_name, vector_dim)

        # ── Build point structs ───────────────────────────────────────
        points: List[models.PointStruct] = []
        for el in embedded:
            payload: Dict = {
                "type": type(el).__name__,
                "metadata": getattr(el, "metadata", {}),
            }
            if isinstance(el, TextElement):
                payload["content"] = el.content
            elif isinstance(el, ImageElement):
                payload["base64_data"] = getattr(el, "base64_data", None)

            points.append(
                models.PointStruct(
                    id=el.element_id,
                    vector={VECTOR_NAME: el.embedding},
                    payload=payload,
                )
            )

        self.client.upsert(collection_name=collection_name, points=points)
        logger.info(
            f"✅ Stored {len(points)} points in '{collection_name}' (dim={vector_dim})"
        )
        return len(points)
============================================================


=== FILE: core/utils/logging.py ===
============================================================
import time
import logging
import functools
from typing import Callable, Any

logger = logging.getLogger("ingestion_pipeline")

def log_node(func: Callable):
    @functools.wraps(func)
    def wrapper(state: Any, *args, **kwargs):
        node_name = func.__name__.replace("node_", "").upper()
        doc_id = state.get("document_id", "unknown")
        
        logger.info(f"[{node_name}] Starting | Doc: {doc_id}")
        start_time = time.perf_counter()
        
        try:
            result = func(state, *args, **kwargs)
            
            end_time = time.perf_counter()
            duration = end_time - start_time
            
            # Context-aware counts
            count = len(result.get("chunks", [])) or len(result.get("elements", []))
            logger.info(f"[{node_name}] Finished | Items: {count} | Time: {duration:.2f}s")
            
            return result
        except Exception as e:
            logger.error(f"[{node_name}] FAILED | Doc: {doc_id} | Error: {str(e)}")
            raise e
            
    return wrapper
============================================================


=== FILE: core/embedders/image.py ===
============================================================
import logging
from typing import List
from PIL import Image
import torch
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)


class ImageEmbedder:
    """CLIP image-only embedder."""

    def __init__(self, model_name: str = "clip-ViT-B-32"):
        self._model_name = model_name
        logger.info(f"Loading CLIP image embedder: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = self.model.to(self.device)

    def embed(self, images: List[Image.Image]) -> List[List[float]]:
        """Batch embed PIL Images."""
        if not images:
            return []
        logger.debug(f"Embedding {len(images)} images with {self.model_name}...")
        embeddings = self.model.encode(
            images,
            batch_size=16,
            show_progress_bar=False,
            normalize_embeddings=True,
            convert_to_numpy=True,
        )
        return embeddings.tolist()
    
    @property
    def dimension(self) -> int:
        return self.model.get_sentence_embedding_dimension()

    @property
    def model_name(self) -> str:
        return self._model_name
============================================================


=== FILE: core/embedders/__init__.py ===
============================================================
from .base import BaseEmbedder
from .text import TextEmbedder
from .image import ImageEmbedder
from .multimodal import MultimodalEmbedder

# Registry for LangGraph nodes to select the embedding engine
EMBEDDER_MAPPING = {
    "text": TextEmbedder,
    "image": ImageEmbedder,
    "multimodal": MultimodalEmbedder
}

def get_embedder(type: str = "text", **kwargs) -> BaseEmbedder:
    """Factory to initialize the requested embedder."""
    embedder_cls = EMBEDDER_MAPPING.get(type.lower())
    if not embedder_cls:
        raise ValueError(f"Unknown embedder type: {type}. Choose from {list(EMBEDDER_MAPPING.keys())}")
    return embedder_cls(**kwargs)

__all__ = ["BaseEmbedder", "TextEmbedder", "ImageEmbedder", "MultimodalEmbedder", "get_embedder"]
============================================================


=== FILE: core/embedders/multimodal.py ===
============================================================
import logging
from typing import List, Union
from PIL import Image
import torch
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)


class MultimodalEmbedder:
    """Unified CLIP embedder – text AND images share the exact same vector space."""

    def __init__(self, model_name: str = "clip-ViT-B-32"):
        self._model_name = model_name
        logger.info(f"Loading unified multimodal CLIP: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = self.model.to(self.device)

    def embed(self, items: List[Union[str, Image.Image]]) -> List[List[float]]:
        """Accepts mixed list: str (text) or PIL.Image (image)."""
        if not items:
            return []

        logger.debug(f"Embedding {len(items)} multimodal items with {self.model_name}...")
        embeddings = self.model.encode(
            items,
            batch_size=16,
            show_progress_bar=False,
            normalize_embeddings=True,
            convert_to_numpy=True,
        )
        return embeddings.tolist()
    
    @property
    def dimension(self) -> int:
        return self.model.get_sentence_embedding_dimension()

    @property
    def model_name(self) -> str:
        return self._model_name
============================================================


=== FILE: core/embedders/base.py ===
============================================================
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseEmbedder(ABC):
    @abstractmethod
    def embed(self, items: List[Any]) -> List[List[float]]:
        """items = list of str (text) or list of PIL.Image (images)"""
        ...

    @property
    @abstractmethod
    def dimension(self) -> int:
        ...

    @property
    @abstractmethod
    def model_name(self) -> str:
        ...
============================================================


=== FILE: core/embedders/text.py ===
============================================================
import logging
from typing import List
from sentence_transformers import SentenceTransformer
from .base import BaseEmbedder

logger = logging.getLogger(__name__)


class TextEmbedder(BaseEmbedder):
    """Text-only embedder using SentenceTransformer (all-MiniLM, etc.)."""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self._model_name = model_name
        logger.info(f"Loading text embedder: {model_name}")
        self.model = SentenceTransformer(model_name)

    def embed(self, texts: List[str]) -> List[List[float]]:
        """Batch embed list of texts."""
        if not texts:
            return []
        logger.debug(f"Embedding {len(texts)} texts with {self.model_name}...")
        embeddings = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=False,
            normalize_embeddings=True,
            convert_to_numpy=True,
        )
        return embeddings.tolist()
    
    @property
    def dimension(self) -> int:
        return self.model.get_sentence_embedding_dimension()

    @property
    def model_name(self) -> str:
        return self._model_name # Ensure this is set in __init__
============================================================


=== FILE: rag/retriever.py ===
============================================================
"""
rag/retriever.py
================
Modernized Qdrant retriever using the Universal Query API (v1.10+).
Features: Store probing, multi-query fusion, and graceful fallbacks.
"""
from __future__ import annotations

import logging
import os
import sys
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any

from qdrant_client import QdrantClient
from qdrant_client import models as qmodels

from .state import RetrievedChunk

logger = logging.getLogger(__name__)

# Must match your ingestion vector name (default is standard for Qdrant)
_PREFERRED_VECTOR_NAME = "default"

@dataclass
class StoreProbeResult:
    collection_exists: bool = False
    point_count: int = 0
    vector_name: Optional[str] = None
    initial_chunks: List[RetrievedChunk] = field(default_factory=list)
    diagnostics: str = ""

class MultiQueryRetriever:
    def __init__(
        self,
        host: str = "localhost",
        port: int = 6333,
        api_key: Optional[str] = None,
        embedder_type: str = "text",
        embedder_model: str = "all-MiniLM-L6-v2",
        top_k: int = 5,
        score_threshold: float = 0.25,
    ):
        # Explicitly disable HTTPS for local Docker setups
        self.client = QdrantClient(host=host, port=port, api_key=api_key, https=False)
        self.top_k = top_k
        self.score_threshold = score_threshold

        # Load Embedder
        sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
        from core.embedders import get_embedder
        self.embedder = get_embedder(type=embedder_type, model_name=embedder_model)
        
        logger.info(f"Retriever Initialized | {embedder_model} | top_k={top_k}")

    def _detect_vector_name(self, collection_name: str) -> Optional[str]:
        """Detects if the collection uses named vectors or a single unnamed vector."""
        try:
            info = self.client.get_collection(collection_name)
            vectors_cfg = info.config.params.vectors
            if isinstance(vectors_cfg, dict):
                return _PREFERRED_VECTOR_NAME if _PREFERRED_VECTOR_NAME in vectors_cfg else next(iter(vectors_cfg))
            return None # Unnamed vector
        except Exception:
            return None

    def _search_safe(
        self,
        embedding: List[float],
        collection_name: str,
        vector_name: Optional[str],
        limit: int,
        threshold: float,
    ) -> List[Any]:
        """
        Executes search using the modern query_points API.
        Falls back to legacy search if query_points is unavailable.
        """
        try:
            # The modern way (Qdrant 1.10+)
            response = self.client.query_points(
                collection_name=collection_name,
                query=embedding,
                using=vector_name,
                limit=limit,
                score_threshold=threshold if threshold > 0 else None,
                with_payload=True
            )
            return response.points
        except AttributeError:
            # Fallback for unexpected environment issues
            logger.warning("query_points not found, falling back to legacy search")
            return self.client.search(
                collection_name=collection_name,
                query_vector=(vector_name, embedding) if vector_name else embedding,
                limit=limit,
                score_threshold=threshold if threshold > 0 else None,
                with_payload=True
            )

    def probe_collection(self, collection_name: str, raw_query: str) -> StoreProbeResult:
        """Verifies store health before agent begins heavy reasoning."""
        result = StoreProbeResult()
        try:
            if not self.client.collection_exists(collection_name):
                result.diagnostics = f"Collection '{collection_name}' missing."
                return result
            
            result.collection_exists = True
            info = self.client.get_collection(collection_name)
            result.point_count = info.points_count or 0
            result.vector_name = self._detect_vector_name(collection_name)

            if result.point_count > 0:
                [emb] = self.embedder.embed([raw_query])
                hits = self._search_safe(emb, collection_name, result.vector_name, self.top_k, self.score_threshold)
                result.initial_chunks = self._hits_to_chunks(hits)
                result.diagnostics = f"Ready. Found {len(result.initial_chunks)} relevant chunks."
            else:
                result.diagnostics = "Collection is empty."
        except Exception as e:
            result.diagnostics = f"Connection Error: {str(e)}"
        
        return result

    def _hits_to_chunks(self, hits: List[Any]) -> List[Dict[str, Any]]:
        """Convert Qdrant hits into a list of dictionaries."""
        return [
            {
                "id": str(hit.id),
                "content": hit.payload.get("content", ""),
                "score": float(hit.score),
                "metadata": hit.payload.get("metadata", {})
            } for hit in hits
        ]

    def retrieve(self, queries: List[str], collection_name: str) -> List[Dict[str, Any]]:
        """Performs multi-query batch retrieval and deduplication."""
        if not queries: return []
        
        v_name = self._detect_vector_name(collection_name)
        embeddings = self.embedder.embed(queries)
        
        all_chunks = {}
        for q_text, emb in zip(queries, embeddings):
            hits = self._search_safe(emb, collection_name, v_name, self.top_k, self.score_threshold)
            for chunk in self._hits_to_chunks(hits):
                # Use bracket notation [] because 'chunk' is a dict
                chunk_id = chunk["id"]
                if chunk_id not in all_chunks or chunk["score"] > all_chunks[chunk_id]["score"]:
                    all_chunks[chunk_id] = chunk

        # Sort by score and cap
        sorted_results = sorted(all_chunks.values(), key=lambda x: x["score"], reverse=True)
        return sorted_results[:self.top_k * 2]
============================================================


=== FILE: rag/__init__.py ===
============================================================

============================================================


=== FILE: rag/config.py ===
============================================================
"""
rag/config.py
=============
Per-request RAG agent configuration.
"""
from __future__ import annotations

from typing import List, Literal, Optional
from pydantic import BaseModel, Field


StrategyName = Literal[
    "query_expansion",
    "query_rewriting",
    "query_decomposition",
    "step_back_prompting",
    "hyde",
    "multi_query",
    "sub_query",
    "auto",     # let the planner choose each iteration
]

RerankMode = Literal["llm", "score"]


class RAGConfig(BaseModel):
    # ── Collection ────────────────────────────────────────────────────────
    collection_name: str = Field("mrag_default", description="Qdrant collection to search")

    # ── Strategy ──────────────────────────────────────────────────────────
    strategy: StrategyName = Field(
        "auto",
        description=(
            "'auto' lets the planner select and rotate strategies each iteration. "
            "Or fix to a single strategy."
        ),
    )
    enabled_strategies: List[str] = Field(
        default=[
            "query_expansion",
            "query_rewriting",
            "query_decomposition",
            "step_back_prompting",
            "hyde",
            "multi_query",
            "sub_query",
        ],
        description="Strategies available to the planner in auto mode.",
    )

    # ── Retrieval ─────────────────────────────────────────────────────────
    top_k: int = Field(5, ge=1, le=20, description="Chunks per query")
    score_threshold: float = Field(0.25, ge=0.0, le=1.0)

    # ── Re-ranking ────────────────────────────────────────────────────────
    rerank_mode: RerankMode = Field("llm", description="'llm' or 'score'")
    rerank_top_n: int = Field(4, ge=1, le=10, description="Chunks kept after reranking")

    # ── Agent loop ────────────────────────────────────────────────────────
    max_iterations: int = Field(5, ge=1, le=10, description="Recursion limit")
    confidence_threshold: float = Field(
        0.75, ge=0.0, le=1.0,
        description="Stop when answer confidence ≥ this value",
    )

    # ── LLM ───────────────────────────────────────────────────────────────
    llm_model: str = Field("openai/gpt-oss-120b", description="Groq model for all agent calls")
    temperature: float = Field(0.0, ge=0.0, le=1.0)
    max_tokens_per_call: int = Field(
        512, ge=64, le=2048,
        description="Max LLM output tokens per call (keeps requests small)",
    )

    # ── Embedding (must match ingestion) ─────────────────────────────────
    embedding_type: Literal["text", "multimodal", "image"] = "text"
    embedding_model: str = "all-MiniLM-L6-v2"

    # ── Final answer ──────────────────────────────────────────────────────
    include_sources: bool = True
    answer_language: str = Field("English", description="Language for the final answer")
============================================================


=== FILE: rag/reranker.py ===
============================================================
"""
rag/reranker.py
===============
Re-ranking layer — sorts retrieved chunks by relevance to the query.

Two modes
---------
"llm"   — ask the LLM to score each chunk (token-efficient: one call, JSON list)
"score" — passthrough, keep Qdrant cosine scores as-is (zero extra tokens)

LLM re-ranking prompt is kept tiny:
  • ≤5 chunks passed at once, each truncated to 120 chars
  • LLM returns only a JSON list of IDs in ranked order
  • Total prompt ≈ 250 tokens
"""
from __future__ import annotations

import json
import logging
import re
from typing import List, Literal

from langchain_core.messages import HumanMessage, SystemMessage

from .state import RetrievedChunk

logger = logging.getLogger(__name__)

RerankMode = Literal["llm", "score"]

_RERANK_SYSTEM = (
    "You are a relevance ranker. Given a query and document snippets, "
    "return ONLY a JSON array of document IDs ordered best-to-worst. "
    "Example: [\"id3\", \"id1\", \"id2\"]"
)


def rerank(
    llm,
    query: str,
    chunks: List[RetrievedChunk],
    mode: RerankMode = "llm",
    top_n: int = 4,
) -> List[RetrievedChunk]:
    """
    Re-rank `chunks` by relevance to `query`.
    Returns at most `top_n` chunks.
    """
    if not chunks:
        return []

    if mode == "score" or len(chunks) <= 1:
        return chunks[:top_n]

    # ── LLM re-ranking ────────────────────────────────────────────────────
    # Build a compact snippet list (max 5 chunks, 120 chars each)
    candidates = chunks[:5]
    snippets = "\n".join(
        f'ID:{c["id"][:8]} | {c["content"][:120].replace(chr(10), " ")}'
        for c in candidates
    )
    prompt = f"Query: {query}\n\nSnippets:\n{snippets}\n\nReturn JSON array of IDs ranked best-first."

    try:
        raw = llm.invoke(
            [SystemMessage(content=_RERANK_SYSTEM), HumanMessage(content=prompt)]
        ).content.strip()

        # Parse JSON — be lenient
        json_match = re.search(r"\[.*?\]", raw, re.DOTALL)
        if json_match:
            ranked_ids = json.loads(json_match.group())
        else:
            raise ValueError("No JSON array found")

        # Map partial IDs back to full IDs
        id_map = {c["id"][:8]: c for c in candidates}
        reranked = []
        for short_id in ranked_ids:
            if short_id in id_map:
                reranked.append(id_map[short_id])
        # Append any not mentioned by LLM at the end
        mentioned = {c["id"] for c in reranked}
        reranked += [c for c in candidates if c["id"] not in mentioned]

        # Append any beyond the 5-candidate window at original scores
        reranked += chunks[5:]

        logger.info(f"[rerank/llm] Reordered {len(reranked)} chunks")
        return reranked[:top_n]

    except Exception as exc:
        logger.warning(f"LLM reranking failed ({exc}) — falling back to score order")
        return chunks[:top_n]


def format_context(chunks: List[RetrievedChunk], max_chars: int = 1600) -> str:
    """
    Format selected chunks into a compact context string for prompt injection.
    Hard-truncated to max_chars to protect token budget.
    """
    parts = []
    total = 0
    for i, c in enumerate(chunks):
        src = c["metadata"].get("source", c["metadata"].get("page", "?"))
        snippet = f"[{i+1}|src:{src}] {c['content']}"
        if total + len(snippet) > max_chars:
            remaining = max_chars - total
            if remaining > 40:
                parts.append(snippet[:remaining] + "…")
            break
        parts.append(snippet)
        total += len(snippet)
    return "\n---\n".join(parts)
============================================================


=== FILE: rag/notepad.py ===
============================================================
"""
rag/notepad.py
==============
Token-efficient notepad and summary manager.

The notepad accumulates structured entries across iterations.
Before each LLM call the full notepad is compressed into a ≤150-token
summary string.  Only that summary (not the raw entries) is sent to the LLM.

This lets the agent run 5–10 iterations while keeping every individual
LLM request small.
"""
from __future__ import annotations

import logging
from typing import List, Optional

from .state import NotepadEntry

logger = logging.getLogger(__name__)

# Hard limits kept intentionally small
MAX_SUMMARY_SENTENCES = 4
MAX_FINDINGS_PER_ENTRY = 80   # chars


def compress_notepad(
    entries: List[NotepadEntry],
    original_query: str,
) -> str:
    """
    Convert a list of NotepadEntry dicts into a short plain-text summary
    that fits in ~150 tokens.

    Structure:
      Query: <original>
      [iter 1] Strategy=<s> | Queries=<n> | <key_findings>
      [iter 2] ...
      Overall: <what we know so far>
    """
    if not entries:
        return f"Query: {original_query}\nNo previous iterations."

    lines: List[str] = [f"Query: {original_query}"]
    for e in entries[-4:]:   # keep at most last 4 entries to cap tokens
        findings = e["key_findings"][:MAX_FINDINGS_PER_ENTRY].rstrip()
        lines.append(
            f"[iter {e['iteration']}] {e['strategy']} | "
            f"{e['chunks_found']} chunks | {findings}"
        )

    # Add a one-liner "what we know"
    if len(entries) > 1:
        all_findings = " ".join(e["key_findings"] for e in entries[-3:])
        # Truncate to ~60 words
        words = all_findings.split()[:60]
        lines.append("Cumulative: " + " ".join(words))

    summary = "\n".join(lines)
    logger.debug(f"Notepad summary ({len(summary.split())} words): {summary[:120]}…")
    return summary


def make_notepad_entry(
    iteration: int,
    strategy: str,
    queries_used: List[str],
    chunks_found: int,
    key_findings: str,
) -> NotepadEntry:
    return NotepadEntry(
        iteration=iteration,
        strategy=strategy,
        queries_used=queries_used,
        chunks_found=chunks_found,
        key_findings=key_findings[:MAX_FINDINGS_PER_ENTRY],
    )


def build_minimal_prompt(
    system_role: str,
    notepad_summary: str,
    current_task: str,
    context_snippet: Optional[str] = None,
) -> str:
    """
    Assemble the smallest useful prompt for a single LLM call.
    Total target: ≤700 tokens.

      [ROLE]          ~80 t
      [MEMORY]        ~150 t
      [TASK]          ~80 t
      [CONTEXT]       ~400 t (optional)
    """
    parts = [
        f"[ROLE] {system_role}",
        f"[MEMORY]\n{notepad_summary}",
        f"[TASK] {current_task}",
    ]
    if context_snippet:
        # Hard-truncate context to ~400 tokens ≈ 1600 chars
        snippet = context_snippet[:1600]
        parts.append(f"[CONTEXT]\n{snippet}")
    return "\n\n".join(parts)
============================================================


=== FILE: rag/strategies.py ===
============================================================
"""
rag/strategies.py
=================
Query transformation strategies.  Each strategy makes ONE small LLM call
and returns a list of query strings for retrieval.

Every call is designed to be ≤400 tokens total (prompt + response).

Strategies
----------
1. query_expansion      — add synonyms / related terms
2. query_rewriting      — rephrase for semantic search
3. query_decomposition  — split complex query into sub-questions
4. step_back_prompting  — abstract to a higher-level question
5. hyde                 — generate a hypothetical document excerpt
6. multi_query          — generate N diverse phrasings
7. sub_query            — identify atomic sub-queries (for multi-hop)
"""
from __future__ import annotations

import logging
import re
from typing import List

from langchain_core.messages import HumanMessage, SystemMessage

logger = logging.getLogger(__name__)

_SYSTEM = (
    "You are a search query specialist. "
    "Respond ONLY with the requested queries, one per line, no numbering, no preamble."
)


def _call(llm, messages) -> str:
    resp = llm.invoke(messages)
    return resp.content.strip()


def _parse_lines(raw: str, max_items: int = 5) -> List[str]:
    lines = [
        re.sub(r"^[\d\.\-\*\s]+", "", ln).strip()
        for ln in raw.splitlines()
        if ln.strip()
    ]
    return [l for l in lines if l][:max_items]


# ─────────────────────────────────────────────────────────────────────────────

def query_expansion(llm, query: str, notepad_summary: str) -> List[str]:
    """Add synonyms and related terms to the original query."""
    prompt = (
        f"Original query: {query}\n\n"
        "Write 3 expanded versions that add relevant synonyms or domain terms. "
        "One per line."
    )
    raw = _call(llm, [SystemMessage(content=_SYSTEM), HumanMessage(content=prompt)])
    result = _parse_lines(raw)
    logger.info(f"[expansion] {len(result)} expanded queries")
    return result or [query]


def query_rewriting(llm, query: str, notepad_summary: str) -> List[str]:
    """Rewrite query for better semantic search retrieval."""
    prompt = (
        f"Original query: {query}\n\n"
        "Rewrite this as 2 different phrasings optimised for dense vector retrieval. "
        "Use noun phrases, avoid stop words. One per line."
    )
    raw = _call(llm, [SystemMessage(content=_SYSTEM), HumanMessage(content=prompt)])
    result = _parse_lines(raw, max_items=3)
    logger.info(f"[rewriting] {len(result)} rewritten queries")
    return result or [query]


def query_decomposition(llm, query: str, notepad_summary: str) -> List[str]:
    """Break a complex query into independent sub-questions."""
    prompt = (
        f"Complex query: {query}\n\n"
        "Break this into 2-4 simpler, independent sub-questions that together answer it. "
        "One per line."
    )
    raw = _call(llm, [SystemMessage(content=_SYSTEM), HumanMessage(content=prompt)])
    result = _parse_lines(raw, max_items=4)
    logger.info(f"[decomposition] {len(result)} sub-questions")
    return result or [query]


def step_back_prompting(llm, query: str, notepad_summary: str) -> List[str]:
    """Abstract the query to a higher-level concept."""
    prompt = (
        f"Specific query: {query}\n\n"
        "Write 1-2 more general / abstract questions that would provide "
        "background knowledge needed to answer the specific query. "
        "One per line."
    )
    raw = _call(llm, [SystemMessage(content=_SYSTEM), HumanMessage(content=prompt)])
    result = _parse_lines(raw, max_items=2)
    # Always include original so we don't drift too far
    return result + [query] if result else [query]


def hyde(llm, query: str, notepad_summary: str) -> List[str]:
    """
    HyDE — generate a hypothetical document excerpt that would answer the query.
    The excerpt itself becomes the retrieval query (embed it, not the text).
    """
    prompt = (
        f"Query: {query}\n\n"
        "Write a short 2-sentence hypothetical document excerpt that would "
        "perfectly answer this query. Be factual and specific."
    )
    raw = _call(llm, [SystemMessage(content=_SYSTEM), HumanMessage(content=prompt)])
    # Return both the hypothetical doc AND the original query
    hypothetical = raw.strip()[:300]
    logger.info(f"[hyde] hypothetical doc: {hypothetical[:80]}…")
    return [hypothetical, query]


def multi_query(llm, query: str, notepad_summary: str) -> List[str]:
    """Generate multiple diverse phrasings of the same question."""
    prompt = (
        f"Query: {query}\n\n"
        "Generate 4 diverse phrasings of this question that explore different angles. "
        "One per line."
    )
    raw = _call(llm, [SystemMessage(content=_SYSTEM), HumanMessage(content=prompt)])
    result = _parse_lines(raw, max_items=4)
    logger.info(f"[multi_query] {len(result)} diverse queries")
    return result or [query]


def sub_query(llm, query: str, notepad_summary: str) -> List[str]:
    """
    Identify atomic sub-queries for multi-hop reasoning.
    Builds on what is already known (notepad_summary) to avoid redundant retrieval.
    """
    memory_hint = ""
    if "No previous" not in notepad_summary:
        memory_hint = f"\nAlready found: {notepad_summary[-200:]}"

    prompt = (
        f"Main query: {query}{memory_hint}\n\n"
        "List 2-3 specific atomic facts that still need to be retrieved to fully answer. "
        "One per line."
    )
    raw = _call(llm, [SystemMessage(content=_SYSTEM), HumanMessage(content=prompt)])
    result = _parse_lines(raw, max_items=3)
    logger.info(f"[sub_query] {len(result)} sub-queries")
    return result or [query]


# ─────────────────────────────────────────────────────────────────────────────
# Strategy registry
# ─────────────────────────────────────────────────────────────────────────────
STRATEGY_FN = {
    "query_expansion": query_expansion,
    "query_rewriting": query_rewriting,
    "query_decomposition": query_decomposition,
    "step_back_prompting": step_back_prompting,
    "hyde": hyde,
    "multi_query": multi_query,
    "sub_query": sub_query,
}

ALL_STRATEGIES = list(STRATEGY_FN.keys())


def run_strategy(
    strategy_name: str,
    llm,
    query: str,
    notepad_summary: str,
) -> List[str]:
    """Dispatch to the right strategy function."""
    fn = STRATEGY_FN.get(strategy_name)
    if fn is None:
        logger.warning(f"Unknown strategy '{strategy_name}' — falling back to original query")
        return [query]
    return fn(llm, query, notepad_summary)
============================================================


=== FILE: rag/state.py ===
============================================================
"""
rag/state.py
============
LangGraph state for the agentic RAG loop.

Token-efficiency design
-----------------------
The LLM never sees the full conversation history.
It only ever receives:
  • system_prompt        (~80 tokens, never changes)
  • notepad_summary      (~150 tokens, compressed rolling summary)
  • current_task         (~80 tokens, what this iteration must do)
  • retrieved_context    (~400 tokens, top-k re-ranked snippets)
  ─────────────────────────────────────────────────────────────
  Total per call        ≈ 700 tokens  (vs thousands in naive RAG)

Full retrieved chunks, intermediate reasoning, etc. live in state
but are NEVER sent raw to the LLM — only the compressed summary is.
"""
from __future__ import annotations

from typing import Annotated, Any, Dict, List, Optional
from typing_extensions import TypedDict


# ── Replace reducer (same as ingestion state) ─────────────────────────────
def _replace(left, right):
    return right if right is not None else left


def _append(left: List, right: Optional[List]) -> List:
    """Append-only reducer for logs / retrieved chunks across iterations."""
    if right is None:
        return left
    return left + right


# ---------------------------------------------------------------------------
class RetrievedChunk(TypedDict):
    id: str
    content: str
    score: float
    metadata: Dict[str, Any]


class NotepadEntry(TypedDict):
    iteration: int
    strategy: str
    queries_used: List[str]
    chunks_found: int
    key_findings: str      # 1-3 sentence compressed finding — what this iteration learned


class RAGState(TypedDict, total=False):
    # ── Inputs (set once) ─────────────────────────────────────────────────
    original_query: str
    collection_name: str
    config: Dict[str, Any]        # RAGConfig serialised to dict

    # ── Agent loop controls ───────────────────────────────────────────────
    iteration: int                # current loop counter
    max_iterations: int           # recursion limit (default 5)
    status: str                   # "running" | "success" | "failed" | "max_iterations"

    # ── Query evolution ───────────────────────────────────────────────────
    active_queries: Annotated[List[str], _replace]   # queries used THIS iteration
    all_queries_tried: Annotated[List[str], _append] # every query ever tried

    # ── Retrieval ─────────────────────────────────────────────────────────
    retrieved_chunks: Annotated[List[RetrievedChunk], _replace]   # current iter chunks
    all_chunks: Annotated[List[RetrievedChunk], _append]          # deduplicated accumulation

    # ── Reranked / selected context ───────────────────────────────────────
    selected_chunks: Annotated[List[RetrievedChunk], _replace]    # after reranking

    # ── Token-efficient memory ────────────────────────────────────────────
    notepad: Annotated[List[NotepadEntry], _append]   # structured iteration log
    notepad_summary: str   # ≤150-token compressed summary sent to LLM each call

    # ── Current iteration plan ────────────────────────────────────────────
    chosen_strategy: str   # which strategy the planner chose this iteration
    current_task: str      # ≤80-token task description sent to LLM

    # ── Answer quality ────────────────────────────────────────────────────
    answer_draft: str      # working answer
    answer_confidence: float   # 0.0–1.0 self-assessed confidence
    final_answer: str          # set only when status = "success"
    answer_sources: List[str]  # source metadata for citations

    # ── Store probe (set by node_probe, read by all subsequent nodes) ────
    store_ready: bool               # True only when collection exists + has points
    vector_name: Optional[str]      # detected Qdrant vector name for this collection
    probe_diagnostics: str          # human-readable probe summary for notepad / UI

    # ── Error tracking ────────────────────────────────────────────────────
    error: Optional[str]
============================================================


=== FILE: rag/nodes.py ===
============================================================
"""
rag/nodes.py
============
LangGraph nodes for the agentic RAG loop.

Execution order
---------------
  init → probe_store → (store empty → synthesize error)
                     → (store ok)  → plan → transform → retrieve → rerank → evaluate
                                     ↑___________________________________|  (loop)
                                                                         → synthesize

probe_store  (new)
  Verifies the collection exists, has points, detects the named-vector field,
  and runs the original query verbatim.  Results seed the first iteration so the
  agent always has *something* before it applies strategies.

node_retrieve  (updated)
  Receives `vector_name` from state (set by probe) — no more guessing.
  Passes probe's initial chunks as a fallback if advanced retrieval returns nothing.

Token budget per node
---------------------
  node_probe       0 LLM   (pure Qdrant + embed)
  node_plan        ~300 t
  node_transform   ~350 t
  node_retrieve    0 LLM
  node_rerank      ~250 t
  node_evaluate    ~600 t
  node_synthesize  ~700 t
"""
from __future__ import annotations

import logging
import os
import re
from functools import lru_cache
from typing import Any, Dict, List, Optional

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_groq import ChatGroq

from .notepad import build_minimal_prompt, compress_notepad, make_notepad_entry
from .reranker import format_context, rerank
from .retriever import MultiQueryRetriever
from .state import RAGState, RetrievedChunk
from .strategies import ALL_STRATEGIES, run_strategy

logger = logging.getLogger(__name__)


# ─────────────────────────────────────────────────────────────────────────────
# Helpers
# ─────────────────────────────────────────────────────────────────────────────
@lru_cache(maxsize=4)
def _get_llm(model: str, temperature: float, max_tokens: int) -> ChatGroq:
    api_key = os.getenv("GROQ_API_KEY", "")
    if not api_key:
        raise ValueError("GROQ_API_KEY not set")
    return ChatGroq(model=model, api_key=api_key,
                    temperature=temperature, max_tokens=max_tokens)


def _llm(cfg: Dict[str, Any]) -> ChatGroq:
    return _get_llm(
        model=cfg.get("llm_model", "llama3-70b-8192"),
        temperature=cfg.get("temperature", 0.0),
        max_tokens=cfg.get("max_tokens_per_call", 512),
    )


def _cfg(state: RAGState) -> Dict[str, Any]:
    return state.get("config") or {}


def _build_retriever(cfg: Dict[str, Any]) -> MultiQueryRetriever:
    """Construct a MultiQueryRetriever from the run config."""
    return MultiQueryRetriever(
        host=cfg.get("qdrant_host", "localhost"),
        port=int(cfg.get("qdrant_port", 6333)),
        api_key=cfg.get("qdrant_api_key"),
        embedder_type=cfg.get("embedding_type", "text"),
        embedder_model=cfg.get("embedding_model", "all-MiniLM-L6-v2"),
        top_k=cfg.get("top_k", 5),
        score_threshold=cfg.get("score_threshold", 0.25),
    )


# ─────────────────────────────────────────────────────────────────────────────
# node_init
# ─────────────────────────────────────────────────────────────────────────────
def node_init(state: RAGState) -> Dict[str, Any]:
    cfg = _cfg(state)
    logger.info(f"[INIT] Query: {state['original_query'][:80]}")
    return {
        "iteration": 0,
        "max_iterations": cfg.get("max_iterations", 5),
        "status": "running",
        "store_ready": False,
        "vector_name": None,
        "probe_diagnostics": "",
        "notepad": [],
        "notepad_summary": f"Query: {state['original_query']}\nNo previous iterations.",
        "all_queries_tried": [],
        "all_chunks": [],
        "answer_confidence": 0.0,
        "answer_draft": "",
    }


# ─────────────────────────────────────────────────────────────────────────────
# node_probe  ★ NEW
# ─────────────────────────────────────────────────────────────────────────────
def node_probe(state: RAGState) -> Dict[str, Any]:
    """
    Verify the vector store before running any strategy.

    1. Check collection exists and is non-empty.
    2. Detect the correct named-vector field — stored in state so every
       subsequent retrieve call uses the right name without re-detecting.
    3. Run the original query verbatim (no transformation) as a baseline.
       If this already yields good chunks the planner knows the store is
       responsive; if scores are low it can choose aggressive strategies.

    Routing: if store is empty / missing, _route_after_probe → "no_store"
             which jumps directly to synthesize with a clear error message.
    """
    cfg = _cfg(state)
    query = state["original_query"]
    collection = cfg.get("collection_name", "mrag_default")

    retriever = _build_retriever(cfg)
    probe = retriever.probe_collection(collection, query)

    logger.info(f"[PROBE] {probe.diagnostics}")

    if not probe.collection_exists or probe.point_count == 0:
        return {
            "store_ready": False,
            "probe_diagnostics": probe.diagnostics,
            "status": "no_store",
        }

    # Seed the first iteration with probe's raw results — they're free
    # (already retrieved) and give the planner a baseline to improve on.
    initial_chunks = probe.initial_chunks or []

    # Build a useful notepad summary so node_plan knows what the probe found
    probe_summary = (
        f"Query: {query}\n"
        f"[probe] direct_search | {len(initial_chunks)} chunks | "
        f"{probe.diagnostics}"
    )

    return {
        "store_ready": True,
        "vector_name": probe.vector_name,
        "probe_diagnostics": probe.diagnostics,
        # Seed state with raw results — the first rerank/evaluate can use these
        "retrieved_chunks": initial_chunks,
        "all_chunks": initial_chunks,       # append reducer
        "notepad_summary": probe_summary,
    }


# ─────────────────────────────────────────────────────────────────────────────
# node_plan
# ─────────────────────────────────────────────────────────────────────────────
def node_plan(state: RAGState) -> Dict[str, Any]:
    cfg = _cfg(state)
    iteration = (state.get("iteration") or 0) + 1
    strategy_pref = cfg.get("strategy", "auto")
    enabled = cfg.get("enabled_strategies", ALL_STRATEGIES)

    if strategy_pref != "auto":
        chosen = strategy_pref
        task = f"Apply {chosen} to the query."
    else:
        llm = _llm(cfg)
        notepad_summary = state.get("notepad_summary", "")
        already_tried = [e["strategy"] for e in (state.get("notepad") or [])]
        remaining = [s for s in enabled if s not in already_tried] or enabled

        prompt = build_minimal_prompt(
            system_role="RAG planner. Pick ONE strategy from the list.",
            notepad_summary=notepad_summary,
            current_task=(
                f"Iteration {iteration}. Available: {', '.join(remaining)}.\n"
                "Reply with ONLY the strategy name."
            ),
        )
        raw = llm.invoke([HumanMessage(content=prompt)]).content.strip().lower()
        chosen = next((s for s in remaining if s in raw), remaining[0])
        task = f"Apply {chosen} to find information not yet retrieved."

    logger.info(f"[PLAN] iter={iteration} strategy={chosen}")
    return {"iteration": iteration, "chosen_strategy": chosen, "current_task": task}


# ─────────────────────────────────────────────────────────────────────────────
# node_transform
# ─────────────────────────────────────────────────────────────────────────────
def node_transform(state: RAGState) -> Dict[str, Any]:
    cfg = _cfg(state)
    llm = _llm(cfg)
    strategy = state.get("chosen_strategy", "query_rewriting")
    query = state["original_query"]
    notepad_summary = state.get("notepad_summary", "")

    queries = run_strategy(strategy, llm, query, notepad_summary)

    # Deduplicate against already-tried queries
    tried = set(state.get("all_queries_tried") or [])
    fresh = [q for q in queries if q not in tried]
    if not fresh:
        fresh = queries

    logger.info(f"[TRANSFORM/{strategy}] {len(fresh)} queries")
    return {"active_queries": fresh}


# ─────────────────────────────────────────────────────────────────────────────
# node_retrieve  (updated — uses pre-detected vector_name from probe)
# ─────────────────────────────────────────────────────────────────────────────

def node_retrieve(state: RAGState) -> Dict[str, Any]:
    cfg = _cfg(state)
    queries = state.get("active_queries") or [state["original_query"]]
    collection = cfg.get("collection_name", "mrag_default")
    
    # We no longer pass vector_name here because the new retriever 
    # detects it automatically via the Universal Query API.
    retriever = _build_retriever(cfg)
    chunks = retriever.retrieve(
        queries=queries,
        collection_name=collection,
    )

    # Fallback: if strategy retrieval returns nothing, reuse probe's raw chunks
    if not chunks:
        fallback = state.get("retrieved_chunks") or []
        if fallback:
            logger.warning(
                "[RETRIEVE] Strategy returned 0 chunks — reusing probe baseline "
                f"({len(fallback)} chunks). Consider lowering score_threshold."
            )
            chunks = fallback

    logger.info(f"[RETRIEVE] {len(chunks)} chunks from {len(queries)} queries")
    return {
        "retrieved_chunks": chunks,
        "all_queries_tried": queries,
        "all_chunks": chunks,
    }
# ─────────────────────────────────────────────────────────────────────────────
# node_rerank
# ─────────────────────────────────────────────────────────────────────────────
def node_rerank(state: RAGState) -> Dict[str, Any]:
    cfg = _cfg(state)
    llm = _llm(cfg)
    chunks = state.get("retrieved_chunks") or []
    query = state["original_query"]
    mode = cfg.get("rerank_mode", "llm")
    top_n = cfg.get("rerank_top_n", 4)

    selected = rerank(llm, query, chunks, mode=mode, top_n=top_n)
    logger.info(f"[RERANK] {len(selected)} chunks selected (mode={mode})")
    return {"selected_chunks": selected}


# ─────────────────────────────────────────────────────────────────────────────
# node_evaluate
# ─────────────────────────────────────────────────────────────────────────────
def node_evaluate(state: RAGState) -> Dict[str, Any]:
    cfg = _cfg(state)
    llm = _llm(cfg)
    iteration = state.get("iteration", 1)
    query = state["original_query"]
    selected = state.get("selected_chunks") or []
    draft = state.get("answer_draft") or ""

    context = format_context(selected, max_chars=1200)
    notepad_summary = state.get("notepad_summary", "")

    task = (
        "Given the context, produce a short answer draft.\n"
        "Then on a new line: CONFIDENCE: <0.0–1.0>\n"
        "Then: KEY_FINDING: <one sentence>\n"
        + (f"Previous draft: {draft[:200]}" if draft else "")
    )

    prompt = build_minimal_prompt(
        system_role="RAG evaluator. Be concise.",
        notepad_summary=notepad_summary,
        current_task=task,
        context_snippet=context,
    )

    raw = llm.invoke([HumanMessage(content=prompt)]).content.strip()

    conf_match = re.search(r"CONFIDENCE[:\s]+([\d.]+)", raw, re.IGNORECASE)
    confidence = float(conf_match.group(1)) if conf_match else 0.5
    confidence = max(0.0, min(1.0, confidence))

    kf_match = re.search(r"KEY_FINDING[:\s]+(.+?)(?:\n|$)", raw, re.IGNORECASE)
    key_finding = kf_match.group(1).strip() if kf_match else raw[:100]

    answer_part = re.split(r"CONFIDENCE[:\s]", raw, flags=re.IGNORECASE)[0].strip()
    new_draft = answer_part if answer_part else draft

    logger.info(f"[EVALUATE] iter={iteration} confidence={confidence:.2f}")

    entry = make_notepad_entry(
        iteration=iteration,
        strategy=state.get("chosen_strategy", "?"),
        queries_used=state.get("active_queries") or [],
        chunks_found=len(selected),
        key_findings=key_finding,
    )
    new_entries = (state.get("notepad") or []) + [entry]
    new_summary = compress_notepad(new_entries, query)

    return {
        "answer_draft": new_draft,
        "answer_confidence": confidence,
        "notepad": [entry],
        "notepad_summary": new_summary,
    }


# ─────────────────────────────────────────────────────────────────────────────
# node_synthesize
# ─────────────────────────────────────────────────────────────────────────────
def node_synthesize(state: RAGState) -> Dict[str, Any]:
    cfg = _cfg(state)
    status = state.get("status", "running")
    query = state["original_query"]

    # ── Hard-fail path: store was empty / missing ─────────────────────────
    if status == "no_store":
        diag = state.get("probe_diagnostics", "No data found in the vector store.")
        logger.warning(f"[SYNTHESIZE] Aborting — store not ready: {diag}")
        return {
            "final_answer": (
                f"❌ Cannot answer: {diag}\n\n"
                "Please ingest documents into the collection before querying."
            ),
            "answer_sources": [],
            "status": "failed",
        }

    llm = _llm(cfg)
    draft = state.get("answer_draft") or ""
    notepad_summary = state.get("notepad_summary", "")
    selected = state.get("selected_chunks") or []
    language = cfg.get("answer_language", "English")
    include_sources = cfg.get("include_sources", True)

    context = format_context(selected, max_chars=1400)

    task = (
        f"Write a complete, well-structured final answer in {language}.\n"
        f"Base it on the context and the draft below.\n"
        f"Draft: {draft[:300]}"
        + ("\nEnd with 'Sources: <list>'." if include_sources else "")
    )

    prompt = build_minimal_prompt(
        system_role="RAG answer synthesizer. Write clearly and cite evidence.",
        notepad_summary=notepad_summary,
        current_task=task,
        context_snippet=context,
    )

    final = llm.invoke([HumanMessage(content=prompt)]).content.strip()

    sources: List[str] = []
    if include_sources:
        for c in selected:
            src = c["metadata"].get("source") or c["metadata"].get("page")
            if src and str(src) not in sources:
                sources.append(str(src))

    if status == "running":
        status = "success"

    logger.info(
        f"[SYNTHESIZE] Done. iters={state.get('iteration',0)} "
        f"conf={state.get('answer_confidence',0):.2f}"
    )
    return {"final_answer": final, "answer_sources": sources, "status": status}
============================================================


=== FILE: rag/graph.py ===
============================================================
"""
rag/graph.py
============
Agentic RAG LangGraph with store probe + recursive loop.

Updated flow
------------

  init → probe_store ──┬── (store empty/missing) ──────────────► synthesize → END
                       │
                       └── (store ok) ──► plan ──► transform ──► retrieve
                                           ▲                          │
                                           │                       rerank
                                           │                          │
                                           └────── evaluate ◄─────────┘
                                                      │
                                           conf ≥ threshold
                                           OR iter = max ──────────► synthesize → END

Why probe first?
  Without it the agent would apply 5 iterations of strategy transformations
  against an empty or misconfigured store and still return nothing.
  The probe gate surfaces the problem immediately with a clear error message.
"""
from __future__ import annotations

import logging
import os
from typing import Any, Dict, Literal, Optional

from langgraph.graph import END, StateGraph
from langgraph.graph.state import CompiledStateGraph

from .nodes import (
    node_evaluate,
    node_init,
    node_plan,
    node_probe,
    node_rerank,
    node_retrieve,
    node_synthesize,
    node_transform,
)
from .state import RAGState

logger = logging.getLogger(__name__)


# ─────────────────────────────────────────────────────────────────────────────
# Routing functions
# ─────────────────────────────────────────────────────────────────────────────
def _route_after_probe(state: RAGState) -> Literal["plan", "synthesize"]:
    """If the store is empty or unreachable, skip straight to synthesize
    which will return a clear error message."""
    if not state.get("store_ready", False):
        logger.warning(
            f"[ROUTE] Store not ready ({state.get('probe_diagnostics')}) → synthesize"
        )
        return "synthesize"
    logger.info("[ROUTE] Store ready → plan")
    return "plan"


def _should_continue(state: RAGState) -> Literal["loop", "synthesize"]:
    """After evaluate: loop or finalize."""
    iteration  = state.get("iteration") or 0
    max_iter   = state.get("max_iterations") or 5
    confidence = state.get("answer_confidence") or 0.0
    threshold  = (state.get("config") or {}).get("confidence_threshold", 0.75)

    if confidence >= threshold:
        logger.info(f"[ROUTE] confidence={confidence:.2f} ≥ {threshold} → synthesize")
        return "synthesize"

    if iteration >= max_iter:
        logger.info(f"[ROUTE] max_iterations={max_iter} hit → synthesize")
        state["status"] = "max_iterations"
        return "synthesize"

    logger.info(f"[ROUTE] iter={iteration}/{max_iter} conf={confidence:.2f} → loop")
    return "loop"


# ─────────────────────────────────────────────────────────────────────────────
# Graph builder
# ─────────────────────────────────────────────────────────────────────────────
def build_rag_graph() -> CompiledStateGraph:
    wf = StateGraph(RAGState)

    wf.add_node("init",       node_init)
    wf.add_node("probe",      node_probe)
    wf.add_node("plan",       node_plan)
    wf.add_node("transform",  node_transform)
    wf.add_node("retrieve",   node_retrieve)
    wf.add_node("rerank",     node_rerank)
    wf.add_node("evaluate",   node_evaluate)
    wf.add_node("synthesize", node_synthesize)

    wf.set_entry_point("init")
    wf.add_edge("init", "probe")

    # Gate: only proceed to planning if the store is usable
    wf.add_conditional_edges(
        "probe",
        _route_after_probe,
        {"plan": "plan", "synthesize": "synthesize"},
    )

    # Main pipeline
    wf.add_edge("plan",      "transform")
    wf.add_edge("transform", "retrieve")
    wf.add_edge("retrieve",  "rerank")
    wf.add_edge("rerank",    "evaluate")

    # Recursive back-edge
    wf.add_conditional_edges(
        "evaluate",
        _should_continue,
        {"loop": "plan", "synthesize": "synthesize"},
    )

    wf.add_edge("synthesize", END)
    return wf.compile()


# ─────────────────────────────────────────────────────────────────────────────
# Public runner
# ─────────────────────────────────────────────────────────────────────────────
async def run_rag(
    query: str,
    config: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    from .config import RAGConfig

    if config is None:
        config = RAGConfig().model_dump()

    config.setdefault("qdrant_host",    os.getenv("QDRANT_HOST", "localhost"))
    config.setdefault("qdrant_port",    int(os.getenv("QDRANT_PORT", "6333")))
    config.setdefault("qdrant_api_key", os.getenv("QDRANT_API_KEY"))

    graph = build_rag_graph()

    initial: RAGState = {
        "original_query":    query,
        "collection_name":   config.get("collection_name", "mrag_default"),
        "config":            config,
        "iteration":         0,
        "max_iterations":    config.get("max_iterations", 5),
        "status":            "running",
        "store_ready":       False,
        "vector_name":       None,
        "probe_diagnostics": "",
        "notepad":           [],
        "notepad_summary":   "",
        "active_queries":    [],
        "all_queries_tried": [],
        "retrieved_chunks":  [],
        "all_chunks":        [],
        "selected_chunks":   [],
        "chosen_strategy":   "",
        "current_task":      "",
        "answer_draft":      "",
        "answer_confidence": 0.0,
        "final_answer":      "",
        "answer_sources":    [],
        "error":             None,
    }
    try:
        from langfuse.langchain import CallbackHandler
        lf_handler = CallbackHandler()
        run_config = {
            "callbacks": [lf_handler],
            "run_name": "Rag",
            "tags": ["rag", "ingestion"],
        }
    except ImportError:
        logger.debug("Langfuse not installed — tracing skipped")

    try:
        final = await graph.ainvoke(initial,config=run_config or None)
        # final["answer_sources"] = []
        # for chunk in final.get("selected_chunks", []):
        #     if chunk.metadata.get('type') == 'image':  # Assuming images have 'type' metadata
        #         final["answer_sources"].append(chunk.metadata.get('source', 'Unknown image path'))
        #     else:
        #         # Full text for text chunks
        #         final["answer_sources"].append(chunk.page_content)

        return final
    except Exception as exc:
        logger.error(f"RAG pipeline crashed: {exc}", exc_info=True)
        return {**initial, "status": "failed", "error": str(exc), "final_answer": ""}
============================================================


=== FILE: frontend/app.py ===
============================================================
"""
frontend/app.py
===============
Stateless Streamlit UI — two tabs:
  1. 📥 Ingest     — upload **multiple** documents into Qdrant
  2. 🔍 RAG Query  — agentic multi-strategy Q&A with live iteration trace + image display

Run:
    streamlit run frontend/app.py

Env:
    API_BASE_URL  (default: http://localhost:8000)
"""
from __future__ import annotations

import json
import os
import time
from typing import Any, Dict, List

import requests
import streamlit as st

API_BASE = os.getenv("API_BASE_URL", "http://localhost:8000")

st.set_page_config(
    page_title="M-RAG", page_icon="🧠",
    layout="wide", initial_sidebar_state="expanded",
)

# ─────────────────────────────────────────────────────────────────────────────
# Data helpers (cached) — unchanged
# ─────────────────────────────────────────────────────────────────────────────
@st.cache_data(ttl=10)
def _ingest_defaults() -> Dict[str, Any]:
    try: return requests.get(f"{API_BASE}/config/defaults", timeout=5).json()
    except: return {}

@st.cache_data(ttl=10)
def _collections() -> List[str]:
    try: return requests.get(f"{API_BASE}/collections", timeout=5).json().get("collections", [])
    except: return []

@st.cache_data(ttl=30)
def _strategies() -> List[str]:
    try: return requests.get(f"{API_BASE}/rag/strategies", timeout=5).json().get("strategies", [])
    except: return ["query_expansion","query_rewriting","query_decomposition",
                    "step_back_prompting","hyde","multi_query","sub_query"]

def _ok() -> bool:
    try: return requests.get(f"{API_BASE}/health", timeout=3).status_code == 200
    except: return False

# ─────────────────────────────────────────────────────────────────────────────
# Sidebar — unchanged
# ─────────────────────────────────────────────────────────────────────────────
with st.sidebar:
    st.title("🧠 M-RAG")
    healthy = _ok()
    if healthy:
        st.success(f"API • {API_BASE}", icon="✅")
    else:
        st.error("API unreachable", icon="🔴")
    if not healthy:
        st.code("uvicorn api.main:app --reload --port 8000")
    st.divider()
    st.caption("Multimodal Agentic RAG\nLangGraph · LangChain · Groq · Qdrant")

# ─────────────────────────────────────────────────────────────────────────────
# Tabs
# ─────────────────────────────────────────────────────────────────────────────
tab_ingest, tab_rag = st.tabs(["📥 Ingest Documents", "🔍 Agentic RAG Query"])

# ══════════════════════════════════════════════════════════════════════════════
# TAB 1 — INGESTION (MULTI-FILE) — unchanged
# ══════════════════════════════════════════════════════════════════════════════
with tab_ingest:
    st.header("📥 Document Ingestion")
    D = _ingest_defaults()
    cfg_col, up_col = st.columns([2, 3])

    with cfg_col:
        st.subheader("⚙️ Pipeline Config")

        describe_imgs = st.toggle("Describe images (Vision LLM)", value=D.get("describe_images", True))
        img_prompt = st.text_area("Image prompt", value=D.get("image_description_prompt","Describe this image in rich detail."), height=60, disabled=not describe_imgs)

        st.divider()
        chunk_method = st.selectbox("Chunking", ["recursive","hierarchical","semantic"],
            index=["recursive","hierarchical","semantic"].index(D.get("chunking_method","recursive")))
        chunk_overlap = st.slider("Overlap (tokens)", 0, 500, D.get("chunk_overlap",100), 10)

        cs = pcs = ccs = sb = None
        if chunk_method == "recursive":
            cs = st.slider("Chunk size", 100, 8000, D.get("chunk_size",1000), 100)
        elif chunk_method == "hierarchical":
            pcs = st.slider("Parent size", 500, 16000, D.get("parent_chunk_size",2000), 100)
            ccs = st.slider("Child size",  100,  4000, D.get("child_chunk_size",800),   50)
        else:
            sb  = st.slider("Breakpoint", 0.0, 1.0, D.get("semantic_breakpoint",0.8), 0.01)

        st.divider()
        emb_type = st.selectbox("Embedder type", ["text","multimodal","image"],
            index=["text","multimodal","image"].index(D.get("embedding_type","text")))
        _opts = {"text":["all-MiniLM-L6-v2","all-mpnet-base-v2"],"multimodal":["clip-ViT-B-32"],"image":["clip-ViT-B-32"]}
        emb_model = st.selectbox("Model", _opts[emb_type])
        custom_emb = st.text_input("Custom model", placeholder="sentence-transformers/…", key="ingest_custom_emb")
        if custom_emb.strip(): emb_model = custom_emb.strip()

        st.divider()
        llm_m    = st.text_input("LLM", value=D.get("llm_model","openai/gpt-oss-120b"), key="ingest_llm")
        vis_m    = st.text_input("Vision LLM", value=D.get("vision_model","meta-llama/llama-4-scout-17b-16e-instruct"), disabled=not describe_imgs, key="ingest_vis")
        cols_c   = _collections()
        coll_n   = st.text_input("Collection", value=D.get("collection_name","mrag_default"), key="ingest_coll")
        if cols_c: st.caption(f"Existing: {', '.join(cols_c)}")

    with up_col:
        st.subheader("📄 Upload Documents")
        uploaded_files = st.file_uploader(
            "Select files to ingest", 
            type=["pdf","txt","md","png","jpg","jpeg"],
            accept_multiple_files=True,
            help="You can select as many PDFs, images, text files, etc. as you want"
        )
        
        if uploaded_files:
            st.info(f"**{len(uploaded_files)} file(s) selected**")
            for f in uploaded_files:
                st.info(f"**{f.name}** • {f.size/1024:.1f} KB")

        ingest_cfg: Dict[str, Any] = dict(
            describe_images=describe_imgs, image_description_prompt=img_prompt,
            chunking_method=chunk_method, chunk_overlap=chunk_overlap,
            embedding_type=emb_type, embedding_model=emb_model,
            llm_model=llm_m, vision_model=vis_m, collection_name=coll_n,
        )
        if cs  is not None: ingest_cfg["chunk_size"]        = cs
        if pcs is not None: ingest_cfg["parent_chunk_size"] = pcs
        if ccs is not None: ingest_cfg["child_chunk_size"]   = ccs
        if sb  is not None: ingest_cfg["semantic_breakpoint"] = sb

        with st.expander("📋 Config (applied to all files)", expanded=False):
            st.json(ingest_cfg)

        go_ingest = st.button(
            "🚀 Ingest All Files", 
            disabled=not(uploaded_files and healthy), 
            type="primary", 
            use_container_width=True
        )

        if go_ingest and uploaded_files:
            st.divider()
            for upload in uploaded_files:
                st.subheader(f"🚀 Processing: **{upload.name}**")
                
                with st.spinner("Submitting…"):
                    try:
                        r = requests.post(
                            f"{API_BASE}/ingest",
                            files={"file": (upload.name, upload.getvalue(), upload.type)},
                            data={"config_json": json.dumps(ingest_cfg)},
                            timeout=45
                        )
                        r.raise_for_status()
                        jid = r.json()["job_id"]
                    except Exception as exc:
                        st.error(f"Submit failed for {upload.name}: {exc}")
                        continue
                
                st.success(f"Job • `{jid}` queued for **{upload.name}**")
                
                bar = st.progress(0, "Queued…")
                stat_ph = st.empty()
                met_ph = st.empty()
                PROG = {"queued":5, "running":50, "success":100, "failed":100}

                for _ in range(300):
                    time.sleep(2)
                    try:
                        d = requests.get(f"{API_BASE}/ingest/{jid}", timeout=10).json()
                    except:
                        continue
                        
                    s = d.get("status", "?")
                    bar.progress(PROG.get(s, 50)/100, s)
                    
                    if s == "success":
                        stat_ph.success(f"✅ Done • `{d.get('document_id')}`")
                        met = d.get("metrics") or {}
                        if met:
                            mc = st.columns(min(len(met), 4))
                            for i, (k, v) in enumerate(met.items()):
                                with mc[i % 4]:
                                    st.metric(
                                        k.replace("_", " ").title(),
                                        f"{v:.2f}s" if isinstance(v, float) and "time" in k else str(v)
                                    )
                        st.success(f"✅ **{upload.name}** successfully ingested!")
                        break
                        
                    elif s == "failed":
                        stat_ph.error(f"❌ {d.get('error')}")
                        break
                    else:
                        stat_ph.info(f"⏳ {s}…")
                
                st.divider()

# ══════════════════════════════════════════════════════════════════════════════
# TAB 2 — AGENTIC RAG (with real image display)
# ══════════════════════════════════════════════════════════════════════════════
with tab_rag:
    st.header("🔍 Agentic RAG Query")
    st.caption(
        "The agent loops recursively—applying query strategies, retrieving, re-ranking—"
        "until confidence ≥ threshold or the iteration limit is reached. "
        "Each LLM call is ≤ ~700 tokens."
    )

    all_strats = _strategies()
    avail_cols = _collections()

    left, right = st.columns([2, 3])

    with left:
        st.subheader("⚙️ Agent Config")
        rag_coll = st.selectbox("Collection", avail_cols or ["mrag_default"], key="rag_coll")

        st.markdown("**Strategy**")
        strat_mode = st.radio("Mode", ["auto (planner picks)", "fixed"], horizontal=True, label_visibility="collapsed", key="strat_mode")

        if strat_mode == "fixed":
            fixed_s = st.selectbox("Fixed strategy", all_strats, key="fixed_strat")
            chosen_s = fixed_s
            enabled_s = all_strats
        else:
            chosen_s = "auto"
            st.caption("Enable strategies for the planner:")
            enabled_s = []
            sc = st.columns(2)
            for i, s in enumerate(all_strats):
                with sc[i % 2]:
                    if st.checkbox(s.replace("_"," ").title(), value=True, key=f"en_{s}"):
                        enabled_s.append(s)
            if not enabled_s:
                enabled_s = all_strats

        with st.expander("ℹ️ Strategy descriptions"):
            _desc = {
                "query_expansion":     "Adds synonyms & domain terms",
                "query_rewriting":     "Rephrases for dense vector search",
                "query_decomposition": "Splits into independent sub-questions",
                "step_back_prompting": "Abstracts to background-level question",
                "hyde":                "Generates hypothetical answer as query",
                "multi_query":         "Creates diverse phrasings of the question",
                "sub_query":           "Identifies still-missing atomic facts",
            }
            for n,d in _desc.items():
                st.markdown(f"**{n.replace('_',' ').title()}** — {d}")

        st.divider()
        st.markdown("**Retrieval & Re-ranking**")
        top_k        = st.slider("Top-K", 1, 20, 5, key="rag_topk")
        score_thr    = st.slider("Score threshold", 0.0, 1.0, 0.25, 0.05, key="rag_thr")
        rerank_mode  = st.radio("Re-rank", ["llm","score"], horizontal=True, key="rag_rerank")
        rerank_top_n = st.slider("Chunks after rerank", 1, 10, 4, key="rag_topn")

        st.divider()
        st.markdown("**Agent Loop**")
        max_iter  = st.slider("Max iterations", 1, 10, 5, key="rag_maxiter")
        conf_thr  = st.slider("Confidence threshold", 0.0, 1.0, 0.75, 0.05, key="rag_conf",
                              help="Agent stops early when confidence ≥ this")

        st.divider()
        st.markdown("**LLM (per-call budget)**")
        rag_llm   = st.text_input("Groq model", "openai/gpt-oss-120b", key="rag_llm")
        max_tok   = st.slider("Max tokens / call", 64, 2048, 512, 64, key="rag_maxtok",
                              help="Keep small to minimize cost per call")
        rag_temp  = st.slider("Temperature", 0.0, 1.0, 0.0, 0.05, key="rag_temp")
        inc_src   = st.checkbox("Include sources", True, key="rag_src")

        rag_cfg = {
            "collection_name": rag_coll,
            "strategy": chosen_s,
            "enabled_strategies": enabled_s,
            "top_k": top_k,
            "score_threshold": score_thr,
            "rerank_mode": rerank_mode,
            "rerank_top_n": rerank_top_n,
            "max_iterations": max_iter,
            "confidence_threshold": conf_thr,
            "llm_model": rag_llm,
            "temperature": rag_temp,
            "max_tokens_per_call": max_tok,
            "include_sources": inc_src,
            "embedding_type": emb_type,
            "embedding_model": emb_model,
        }
        with st.expander("📋 Config JSON", expanded=False):
            st.json(rag_cfg)

    with right:
        st.subheader("💬 Ask")
        query_txt = st.text_area(
            "Question", placeholder="e.g. What are the main conclusions about transformer scaling?",
            height=110, key="rag_query",
        )

        if max_iter and max_tok:
            est_tokens = (max_iter * (300+350+250+600) + 700) * (max_tok / 512)
            st.caption(
                f"💰 Estimated token budget: ~{int(est_tokens):,} tokens total "
                f"({max_iter} iter × ~1 500 t/iter + synthesis)"
            )

        go_rag = st.button("🔍 Run Agent", disabled=not(query_txt.strip() and healthy),
                           type="primary", use_container_width=True, key="go_rag")

        if go_rag and query_txt.strip():
            with st.spinner("Submitting…"):
                try:
                    r = requests.post(f"{API_BASE}/rag/query",
                        json={"query": query_txt, "config": rag_cfg}, timeout=30)
                    r.raise_for_status()
                    rjid = r.json()["job_id"]
                except Exception as exc:
                    st.error(f"Submit failed: {exc}"); st.stop()

            st.success(f"Job • `{rjid}`")

            rag_prog   = st.progress(0, "Starting…")
            probe_ph   = st.empty()
            rag_meta   = st.empty()
            rag_trace  = st.empty()
            rag_answer = st.empty()
            rag_srcs   = st.empty()

            DONE = {"success","failed","max_iterations","no_store"}
            last_shown   = 0
            probe_shown  = False

            for tick in range(600):
                time.sleep(2)
                try:
                    rd = requests.get(f"{API_BASE}/rag/query/{rjid}", timeout=10).json()
                except: continue

                rs         = rd.get("status","running")
                iters      = rd.get("iterations_used") or 0
                conf       = rd.get("answer_confidence") or 0.0
                notepad    = rd.get("notepad") or []
                answer     = rd.get("final_answer") or ""
                sources    = rd.get("answer_sources") or []
                probe_diag = rd.get("probe_diagnostics","")

                if probe_diag and not probe_shown:
                    probe_shown = True
                    if rs in ("failed","no_store") or "does not exist" in probe_diag or "0 points" in probe_diag:
                        probe_ph.error(f"🔴 **Store probe:** {probe_diag}")
                    else:
                        probe_ph.success(f"🟢 **Store probe:** {probe_diag}")

                if rs in DONE:
                    rag_prog.progress(1.0, f"Done — {rs}")
                else:
                    rag_prog.progress(min(0.9, iters/max_iter * 0.85 + 0.05),
                                      f"Iter {iters}/{max_iter}  •  confidence {conf:.0%}")

                icon = "✅" if rs=="success" else ("⚠️" if rs=="max_iterations" else ("❌" if rs in("failed","no_store") else "⏳"))
                rag_meta.markdown(
                    f"{icon} **Status:** `{rs}` &nbsp;|&nbsp; "
                    f"**Iterations:** {iters}/{max_iter} &nbsp;|&nbsp; "
                    f"**Confidence:** {conf:.0%}"
                )

                if notepad and len(notepad) > last_shown:
                    last_shown = len(notepad)
                    with rag_trace.container():
                        st.markdown("---")
                        st.markdown("#### 📓 Agent Reasoning Trace")
                        for entry in notepad:
                            with st.expander(
                                f"**Iter {entry['iteration']}** · "
                                f"{entry['strategy'].replace('_',' ').title()} · "
                                f"{entry['chunks_found']} chunks retrieved",
                                expanded=(entry['iteration'] == len(notepad)),
                            ):
                                qs = entry.get("queries_used", [])
                                if qs:
                                    st.markdown("**Queries sent to Qdrant:**")
                                    for q in qs:
                                        st.code(q, language=None)
                                st.info(f"**Key finding:** {entry['key_findings']}")

                # ── FINAL ANSWER + IMAGE DISPLAY ─────────────────────────────────────
                if rs in DONE and answer:
                    with rag_answer.container():
                        st.markdown("---")
                        if rs == "max_iterations":
                            st.warning(f"⚠️ Hit iteration limit ({max_iter}). Best available answer:")
                        elif rs in ("failed","no_store"):
                            st.error(f"Pipeline error: {rd.get('error') or answer}")
                        else:
                            st.success(f"Answer generated in {iters} iteration(s) with {conf:.0%} confidence")
                        st.markdown("### 💡 Final Answer")
                        st.markdown(answer)

                    # ── IMPROVED SOURCES (now shows full text!) ─────────────────
                    if sources:
                        with rag_srcs.container():
                            st.markdown("---")
                            st.markdown("### 📚 Sources")
                            st.caption("Retrieved chunks and **images** used by the agent")

                            # Temporary debug (remove after you confirm it works)
                            with st.expander("🔍 Debug: Raw sources from backend", expanded=False):
                                st.json(sources)

                            for i, src in enumerate(sources, 1):
                                if isinstance(src, str) and src.lower().endswith(('.png','.jpg','.jpeg','.webp','.gif')):
                                    # IMAGE
                                    st.markdown(f"**Source {i}** — 🖼️ Image")
                                    img_url = src if src.startswith(('http://','https://')) else f"{API_BASE.rstrip('/')}/{src.lstrip('/')}"
                                    try:
                                        st.image(img_url, use_column_width=True)
                                        st.caption(f"`{src}`")
                                    except Exception:
                                        st.warning(f"Could not load image (check path):")
                                        st.code(src)

                                else:
                                    # TEXT CHUNK (handles string, int, dict — now shows real text)
                                    title = f"**Source {i}** — Text chunk"
                                    content = ""

                                    if isinstance(src, dict):
                                        content = (
                                            src.get("text") or
                                            src.get("page_content") or
                                            src.get("content") or
                                            src.get("chunk") or
                                            str(src)
                                        )
                                        score = src.get("score") or src.get("relevance")
                                        if score:
                                            title += f" [score: {score:.3f}]"
                                    else:
                                        content = str(src)

                                    with st.expander(title, expanded=True):
                                        st.markdown(content)

                    break
============================================================


=== FILE: ingestion/nodes.py ===
============================================================
"""
graph/nodes.py
==============
Pure execution nodes for the M-RAG ingestion pipeline.

Fixes applied vs original:
  1. next_step removed from every return dict  →  routing lives in graph.py only.
  2. status = "success" set explicitly in node_store  →  callers can rely on it.
  3. Hierarchical children are FLATTENED before embedding  →  every chunk gets
     an embedding and is stored individually (parents + children both stored).
  4. Image replacement uses element_id set (bulletproof, works with new reducer).
  5. model_name / vision_model pulled from config  →  fully runtime-configurable.
  6. Metrics always merged with existing dict (no silent loss).
"""
from __future__ import annotations

import logging
import time
from typing import Any, Dict, List

from core.state import IngestionState
from core.loaders import get_loader_for_file
from core.processors.image_describer import ImageDescriber
from core.chunkers import get_chunker
from core.embedders import get_embedder
from core.storers.qdrant import QdrantStorer
from core.utils.logging import log_node
from core.elements import BaseElement, TextElement, ImageElement

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------
def _merge_metrics(state: IngestionState, new: Dict[str, Any]) -> Dict[str, Any]:
    """Merge new metric keys into the existing metrics dict."""
    base = dict(state.get("metrics") or {})
    base.update(new)
    return base


def _flatten_elements(elements: List[BaseElement]) -> List[BaseElement]:
    """
    Flatten hierarchical parent→children into a single list.
    Parents are kept (they carry the full-context text); children are appended
    so every chunk gets embedded and stored.

    Fix for: "Hierarchical Chunking Children Are Never Embedded or Stored"
    """
    flat: List[BaseElement] = []
    for el in elements:
        flat.append(el)
        if el.children:
            flat.extend(el.children)
            el.children = []   # prevent double-processing on re-runs
    return flat


# ---------------------------------------------------------------------------
# Nodes
# ---------------------------------------------------------------------------
@log_node
def node_load(state: IngestionState) -> Dict[str, Any]:
    """Load raw elements from file (PDF, TXT, MD, PNG, JPG, …)."""
    file_path: str = state["file_path"]
    start = time.perf_counter()

    loader = get_loader_for_file(file_path)
    raw_elements = loader.load(file_path)

    duration = time.perf_counter() - start
    logger.info(f"Loaded {len(raw_elements)} raw elements in {duration:.2f}s")

    return {
        "elements": raw_elements,
        "metrics": _merge_metrics(state, {"load_time": duration, "raw_elements": len(raw_elements)}),
    }


@log_node
def node_describe_images(state: IngestionState) -> Dict[str, Any]:
    """
    Vision-LLM image description + replacement.

    Uses element_id set for safe removal even after reducer transforms.
    Skips gracefully when no images are present.
    """
    elements: List[BaseElement] = state.get("elements", [])
    config: Dict[str, Any] = state.get("config", {})

    image_elements = [el for el in elements if isinstance(el, ImageElement)]
    if not image_elements:
        logger.info("No images to describe → skipping")
        return {}   # empty dict → no state mutation

    describer = ImageDescriber(
        prompt=config.get("image_description_prompt"),
        vision_model=config.get("vision_model"),        # runtime-configurable
    )

    start = time.perf_counter()
    described_texts = describer.describe(image_elements)
    duration = time.perf_counter() - start

    # Replace image elements with their text descriptions (safe by ID)
    image_ids = {img.element_id for img in image_elements}
    non_image = [el for el in elements if el.element_id not in image_ids]
    new_elements = non_image + described_texts

    logger.info(
        f"Described {len(image_elements)} images → {len(described_texts)} "
        f"TextElements in {duration:.2f}s"
    )
    return {
        "elements": new_elements,
        "metrics": _merge_metrics(
            state, {"describe_time": duration, "images_described": len(image_elements)}
        ),
    }


@log_node
def node_chunk(state: IngestionState) -> Dict[str, Any]:
    """
    Chunk elements with the method specified in config.
    Only passes kwargs the selected chunker actually accepts.
    """
    elements: List[BaseElement] = state.get("elements", [])
    config: Dict[str, Any] = state.get("config", {})
    method: str = config.get("chunking_method", "recursive").lower()

    kwargs: Dict[str, Any] = {"chunk_overlap": config.get("chunk_overlap", 100)}

    if method == "hierarchical":
        kwargs.update(
            {
                "parent_chunk_size": config.get("parent_chunk_size", 2000),
                "child_chunk_size": config.get("child_chunk_size", 800),
            }
        )
    elif method == "recursive":
        kwargs["chunk_size"] = config.get("chunk_size", 1000)
    elif method == "semantic":
        kwargs["breakpoint_threshold"] = config.get("semantic_breakpoint", 0.8)

    logger.debug(f"Creating '{method}' chunker with kwargs: {list(kwargs.keys())}")
    chunker = get_chunker(method=method, **kwargs)

    start = time.perf_counter()
    chunks = chunker.chunk(elements)
    duration = time.perf_counter() - start

    # ── Fix: flatten hierarchical parent→child before proceeding ──────────
    flat_chunks = _flatten_elements(chunks)

    logger.info(
        f"Chunked → {len(chunks)} chunks (flattened to {len(flat_chunks)}) "
        f"using '{method}' in {duration:.2f}s"
    )
    return {
        "elements": flat_chunks,
        "metrics": _merge_metrics(
            state,
            {
                "chunk_time": duration,
                "chunks_pre_flatten": len(chunks),
                "chunks": len(flat_chunks),
                "chunking_method": method,
            },
        ),
    }


@log_node
def node_embed(state: IngestionState) -> Dict[str, Any]:
    """
    Embed all elements using the embedder specified in config.

    Fix: embedder model is pulled from config so it is fully runtime-configurable.
    The Qdrant storer already reads vector dimension from elements, so changing
    the model here automatically propagates a correct collection schema.
    """
    elements: List[BaseElement] = state.get("elements", [])
    config: Dict[str, Any] = state.get("config", {})

    embedder_type: str = config.get("embedding_type", "text")
    embedder_model: str = config.get("embedding_model", "all-MiniLM-L6-v2")

    embedder = get_embedder(type=embedder_type, model_name=embedder_model)

    # Build input list — text content or base64 fallback
    items_to_embed: List[Any] = []
    for el in elements:
        if hasattr(el, "content") and isinstance(el.content, str) and el.content.strip():
            items_to_embed.append(el.content)
        elif hasattr(el, "base64_data") and el.base64_data:
            items_to_embed.append(el.metadata.get("description", "image"))
        else:
            items_to_embed.append(str(el.content) if el.content else "")

    start = time.perf_counter()
    embeddings = embedder.embed(items_to_embed)
    duration = time.perf_counter() - start

    for el, emb in zip(elements, embeddings):
        el.embedding = emb
        el.metadata["embedding_model"] = embedder.model_name
        el.metadata["embedding_dim"] = embedder.dimension

    logger.info(
        f"Embedded {len(elements)} chunks with '{embedder_type}' "
        f"({embedder.model_name}, dim={embedder.dimension}) in {duration:.2f}s"
    )
    return {
        "elements": elements,
        "metrics": _merge_metrics(
            state,
            {
                "embed_time": duration,
                "embedder": embedder.model_name,
                "embedding_dim": embedder.dimension,
            },
        ),
    }


@log_node
def node_store(state: IngestionState) -> Dict[str, Any]:
    """
    Store everything to Qdrant.

    Fix: sets status = "success" so callers always get a deterministic signal.
    The QdrantStorer now derives vector config from the actual embedding dimension
    on the elements (configurable embedder fix).
    """
    from core.settings import get_settings   # lazy import to avoid circular

    elements: List[BaseElement] = state.get("elements", [])
    config: Dict[str, Any] = state.get("config", {})
    settings = get_settings()

    collection: str = config.get("collection_name", settings.default_collection)

    storer = QdrantStorer(
        host=config.get("qdrant_host", settings.qdrant_host),
        port=config.get("qdrant_port", settings.qdrant_port),
        api_key=config.get("qdrant_api_key", settings.qdrant_api_key),
    )

    start = time.perf_counter()
    stored_count = storer.store(elements, collection_name=collection)
    duration = time.perf_counter() - start

    logger.info(f"Stored {stored_count} vectors to '{collection}' in {duration:.2f}s")

    return {
        "status": "success",
        "metrics": _merge_metrics(
            state,
            {"store_time": duration, "stored": stored_count, "collection": collection},
        ),
    }
============================================================


=== FILE: ingestion/graph.py ===
============================================================
"""
graph/graph.py
==============
LangGraph ingestion pipeline — clean conditional routing.

Fixes applied vs original:
  1. next_step removed from state entirely.  Routing functions inspect
     config directly so the graph is the single source of routing truth.
  2. node_router removed (was vestigial once next_step is gone).
  3. Langfuse callback wired via run_config, not a module-level singleton,
     so it picks up per-request metadata cleanly.
"""
from __future__ import annotations

import logging
import os
import time
from typing import Any, Dict, Literal, Optional

from dotenv import load_dotenv
from langgraph.graph import END, StateGraph
from langgraph.graph.state import CompiledStateGraph

from core.state import IngestionState
from .nodes import node_load, node_describe_images, node_chunk, node_embed, node_store

load_dotenv()
logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Routing functions — inspect config, NOT state.next_step
# ---------------------------------------------------------------------------
def _route_after_load(state: IngestionState) -> Literal["describe_images", "chunk"]:
    """Skip image description when disabled in config or when no images exist."""
    config = state.get("config", {})
    if not config.get("describe_images", True):
        return "chunk"
    # Check if any ImageElement was actually loaded
    from core.elements import ImageElement
    elements = state.get("elements", [])
    if any(isinstance(el, ImageElement) for el in elements):
        return "describe_images"
    return "chunk"


# ---------------------------------------------------------------------------
# Graph builder
# ---------------------------------------------------------------------------
def build_ingestion_graph() -> CompiledStateGraph:
    """Build and compile the M-RAG ingestion graph."""
    workflow = StateGraph(IngestionState)

    workflow.add_node("load", node_load)
    workflow.add_node("describe_images", node_describe_images)
    workflow.add_node("chunk", node_chunk)
    workflow.add_node("embed", node_embed)
    workflow.add_node("store", node_store)

    workflow.set_entry_point("load")

    # Conditional branch after load
    workflow.add_conditional_edges(
        "load",
        _route_after_load,
        {"describe_images": "describe_images", "chunk": "chunk"},
    )

    # Linear tail
    workflow.add_edge("describe_images", "chunk")
    workflow.add_edge("chunk", "embed")
    workflow.add_edge("embed", "store")
    workflow.add_edge("store", END)

    return workflow.compile()


# ---------------------------------------------------------------------------
# Public runner
# ---------------------------------------------------------------------------
async def run_ingestion(
    file_path: str,
    config: Optional[Dict[str, Any]] = None,
    run_name: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Async entry point for a single ingestion job.

    Parameters
    ----------
    file_path : str
        Absolute path to the file to ingest.
    config : dict, optional
        Serialised PipelineConfig (call `.model_dump()` before passing).
    run_name : str, optional
        Human-readable label for Langfuse tracing.
    """
    from core.config import PipelineConfig
    from core.settings import get_settings

    settings = get_settings()
    document_id = os.path.basename(file_path).replace(".", "_").replace(" ", "_")
    start = time.perf_counter()

    logger.info(f"🚀 Starting ingestion for: {document_id}")

    if config is None:
        config = PipelineConfig.from_settings(settings).model_dump()

    graph = build_ingestion_graph()

    initial_state: IngestionState = {
        "file_path": file_path,
        "document_id": document_id,
        "elements": [],
        "config": config,
        "metrics": {},
        "status": "running",
        "error": None,
    }

    # ── Langfuse tracing (optional — skipped if env vars absent) ──────────
    run_config: Dict[str, Any] = {}
    try:
        from langfuse.langchain import CallbackHandler
        lf_handler = CallbackHandler()
        run_config = {
            "callbacks": [lf_handler],
            "run_name": run_name or f"Ingest_{document_id}",
            "tags": ["rag", "ingestion"],
            "metadata": {"file_path": file_path, "document_id": document_id},
        }
    except ImportError:
        logger.debug("Langfuse not installed — tracing skipped")

    try:
        final_state = await graph.ainvoke(initial_state, config=run_config or None)
        duration = time.perf_counter() - start
        logger.info(
            f"✅ Ingestion complete | Doc: {document_id} | "
            f"Status: {final_state.get('status')} | Time: {duration:.2f}s"
        )
        return final_state

    except Exception as exc:
        duration = time.perf_counter() - start
        logger.error(f"❌ Pipeline crashed for {document_id}: {exc}", exc_info=True)
        return {
            **initial_state,
            "status": "failed",
            "error": str(exc),
            "metrics": {**(initial_state.get("metrics") or {}), "total_time": duration},
        }
============================================================
